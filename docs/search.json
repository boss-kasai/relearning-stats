[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "記事一覧",
    "section": "",
    "text": "記事一覧\n\nRブログはじめました\nRでRのブログを作る\nRの組み込みデータ\n基礎統計量と欠損値の確認\n基本的なデータの可視化をしてみる\nt検定をする前に\nt検定\n分散分析 - 導入と独立一元配置\n分散分析 - 独立二元配置\n分散分析 - 繰り返しのある二元配置\nラボラトリー方式はラボを出られるか"
  },
  {
    "objectID": "column.html",
    "href": "column.html",
    "title": "コラム",
    "section": "",
    "text": "プラグラミングのコードなどではない記事です。\n\n\n\nRブログはじめました\nRでRのブログを作る\nラボラトリー方式はラボを出られるか"
  },
  {
    "objectID": "column.html#記事一覧",
    "href": "column.html#記事一覧",
    "title": "コラム",
    "section": "",
    "text": "Rブログはじめました\nRでRのブログを作る\nラボラトリー方式はラボを出られるか"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "relearning-stats",
    "section": "",
    "text": "relearning-stats にようこそ！\nこのブログは８年ぶりにRでの分析の勉強をしている人のブログです。 以前は『心理系の大学生が、Rをそこそこ使えるようになるサイトを目指して・・・』というブログを書いていましたが、ここからリスタートです。\n統計の専門家というわけではないので、情報に誤りがあることもあります。 参考程度にしてください。\n基本的にはRを使っていこうと思いますが、せっかくなのでPythonとかも使ってみたいと思っています。\n各記事へのリンクはヘッダーから探してください。\nブログの機能拡張も考えていますが、まだ使いづらくてすみません。"
  },
  {
    "objectID": "index.html#お願い",
    "href": "index.html#お願い",
    "title": "relearning-stats",
    "section": "お願い",
    "text": "お願い\nいつでも面白いサンプルデータを求めています。 「こんなデータが公開されているよ」とかって情報がありましたら、教えていただきたいです。\nお問い合わせフォームはこちら"
  },
  {
    "objectID": "posts/2025-4-18-before-t-test.html",
    "href": "posts/2025-4-18-before-t-test.html",
    "title": "t検定をする前に",
    "section": "",
    "text": "t検定のことを書こうと思ったのですが、よく考えたらt検定の前に事前に確認するべきことがあったので、今回はそのお話です。\n実際の分析の前にやった方が良いことはたくさんあると思います。 基礎統計量を見る 欠損値の確認 データの可視化 などなど\n今回はt検定前に見た方が良いかなと思う下記３つを取り上げてみます。"
  },
  {
    "objectID": "posts/2025-4-18-before-t-test.html#正規性",
    "href": "posts/2025-4-18-before-t-test.html#正規性",
    "title": "t検定をする前に",
    "section": "正規性",
    "text": "正規性\n正規性とはデータが正規分布に従っている(近い)かどうかを意味します。 正規分布は、平均を中心に左右対称な釣鐘型の分布になってるやつです。 t検定や分散分析は正規分布を前提にしている分析手法なので、正規分布をしていない場合は他の分析方法を検討することになったりします。 外れ値が極端に多かったり、天井効果などは想像しやすい例かもしれません。\n今回は、有名なシャピロ・ウィルク(Shapiro-Wilk)検定をやってみます。\n\nshapiro.test(ToothGrowth$len[ToothGrowth$supp == \"OJ\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  ToothGrowth$len[ToothGrowth$supp == \"OJ\"]\nW = 0.91784, p-value = 0.02359\n\n\n１行ずつ見て行きましょう。\nShapiro-Wilk normality test\nShapiro-Wilk検を実施しましたよってことです。\ndata:  ToothGrowth$len[ToothGrowth$supp == \"OJ\"]\nToothGrowth データの中の、supp（サプリメント）が “OJ” の行だけを抜き出した、len（歯の長さ）のデータを検定対象にしたことを示しています。\nW = 0.91784, p-value = 0.02359\n\nW = 0.91784 Shapiro-Wilk検定統計量（正規性にどれだけ一致するかの指標）\np-value = 0.02359 p値：正規分布からの逸脱が偶然で起こる確率\n\np値が0.05未満なので、このToothGrowth$len[ToothGrowth$supp == \"OJ\"]は正規分布していないということになります。 t検定の記事でこのデータを使ってしまっていましたが、実際は不適切だったかもしれません。 そんな時はノンパラメトリック検定をやるんだったと思います。 なんとなくしか覚えていないので、ノンパラメトリック検定についてはまた別記事で。\nとりあえず、ToothGrowth$len[ToothGrowth$supp == \"VC\"]も正規性を確認しておきましょう。\n\nshapiro.test(ToothGrowth$len[ToothGrowth$supp == \"VC\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  ToothGrowth$len[ToothGrowth$supp == \"VC\"]\nW = 0.96567, p-value = 0.4284\n\n\n各行の解説は省略しますが、p値が0.05以上なのでこちらは正規性があったということで、正規分布とみなせます。"
  },
  {
    "objectID": "posts/2025-4-18-before-t-test.html#等分散性",
    "href": "posts/2025-4-18-before-t-test.html#等分散性",
    "title": "t検定をする前に",
    "section": "等分散性",
    "text": "等分散性\n次は等分散性を見て行きましょう。 「等分散性（とうぶんさんせい、equal variance）」とは、「比較する複数のグループでデータのばらつき（＝分散）が同じくらいかどうか」を意味します。\np &gt; 0.05 → 分散が等しい（var.equal = TRUE を指定しても良い）\n\nvar.test(len ~ supp, data = ToothGrowth)\n\n\n    F test to compare two variances\n\ndata:  len by supp\nF = 0.6386, num df = 29, denom df = 29, p-value = 0.2331\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3039488 1.3416857\nsample estimates:\nratio of variances \n         0.6385951 \n\n\nさて、実行結果を１行ずつ確認して行きましょう。\nF test to compare two variances\n2つのグループ（ここでは “OJ” vs “VC”）の 分散が等しいかどうか を比較する F検定（F test）を実施したことを示しています。\ndata:  len by supp\n検定対象データがlenという変数をsuppでグループ分けしたことが出力されています。\nF = 0.6386, num df = 29, denom df = 29, p-value = 0.2331\n\nF = 0.6386 分散の比（大きい分散 / 小さい分散）に基づいて計算されたF値。1に近いほど分散が等しい。\nnum df = 29, denom df = 29 自由度：各グループのデータ数（30）−1 = 29\np-value = 0.2331 p値（有意確率）。\n\n今回の分析結果から、p値が0.05より大きいので、「分散に有意な差はない（＝等分散性あり）」と判断することができます。これならstudentのt検定ができそうですね。まぁ、正規性に問題があるのですが笑"
  },
  {
    "objectID": "posts/2025-4-18-before-t-test.html#外れ値",
    "href": "posts/2025-4-18-before-t-test.html#外れ値",
    "title": "t検定をする前に",
    "section": "外れ値",
    "text": "外れ値\n最後は外れ値に関してみて行きます。 外れ値に関しては、箱ひげ図で見るのが簡単だと思います。 箱ひげ図で点（○）があれば外れ値の可能性ありです。\nboxplot() 関数では外れ値（outlier）は自動で統計的に判定されています。 この判定には、「IQR（四分位範囲）に基づくルール」が使われています。\n外れ値 = 以下のどちらかに該当する値\n  ・Q1（第1四分位数）よりも小さい → Q1 - 1.5 × IQR\n  ・Q3（第3四分位数）よりも大きい → Q3 + 1.5 × IQR\n\nIQR Interquartile Range（四分位範囲）＝ Q3 - Q1\nまずは、ここまで使用してきたToothGrowthのデータを見てみましょう。 シンプルな箱ひげ図で見てみましょう。 図があるので先にコードの説明をします。 - boxplot 箱ひげ図を作成する関数です。 - len ~ supp　lenのデータをsuppでグループ分けして、図を作成します。 - data = ToothGrowth　使用するデータを指定しています。\n\nboxplot(len ~ supp, data = ToothGrowth)\n\n\n\n\n\n\n\n\n箱ひげ図に点がないので、外れ値はなさそうです。 せっかくなので、外れ値のあるデータも見てみましょう。 cars という、speed(スピード)とdist(制動距離)についてのデータです(気になる方は、データを詳しくみてください)。 実際に箱ひげ図を作成してみましょう。 コードの解説\nboxplot(データフレーム名$列名)\n\nboxplot(cars$dist)\n\n\n\n\n\n\n\n\n120の部分に点が表示されています。 これが外れ値です。 外れ値の原因や処理方法はデータを詳しくみて判断するしかないと思います。\n外れ値の確認は一旦こんな感じでいいでしょう。 データの可視化に関しては、深くて面白い世界なのでまた別で取り上げられればと思います。 t検定など分析をやる前には、データを事前にしっかり見ておきたいですね。 では、今回はこの辺で"
  },
  {
    "objectID": "posts/2025-04-16-basic-stats-na-check.html",
    "href": "posts/2025-04-16-basic-stats-na-check.html",
    "title": "基礎統計量と欠損値の確認",
    "section": "",
    "text": "今回やることは”基礎統計量と欠損値の確認”です。 irisデータを用いて進めていきます。"
  },
  {
    "objectID": "posts/2025-04-16-basic-stats-na-check.html#今回やること",
    "href": "posts/2025-04-16-basic-stats-na-check.html#今回やること",
    "title": "基礎統計量と欠損値の確認",
    "section": "",
    "text": "今回やることは”基礎統計量と欠損値の確認”です。 irisデータを用いて進めていきます。"
  },
  {
    "objectID": "posts/2025-04-16-basic-stats-na-check.html#基礎統計量",
    "href": "posts/2025-04-16-basic-stats-na-check.html#基礎統計量",
    "title": "基礎統計量と欠損値の確認",
    "section": "基礎統計量",
    "text": "基礎統計量\nまずはデータ構造を見てみましょう。\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nこの出力から、下記のことが分かります。 150 obs : 150行 5 variables: 5列\n$ Sepal.Length: num　はSepal.Length列がnum型のデータであることが分かります。 $ Species : Factor w/ 3 levels　はSpecies列がFactor型で３つの種類を持っていることが分かります。\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\n統計をかじっている方なら馴染み深い基礎統計量が出てきました。\n\nMin. 最小値（最も小さい値）\n1st Qu. 第1四分位数（下位25%の境目）\nMedian 中央値（50%の地点）\nMean 平均値（算術平均）\n3rd Qu. 第3四分位数（上位25%の境目）\nMax. 最大値（最も大きい値）\n\nSpeciesはFactor型なので、各カテゴリが何件ずつ登録されているかが表示されています。\nサンプルデータだけあって、綺麗なデータですね。"
  },
  {
    "objectID": "posts/2025-04-16-basic-stats-na-check.html#欠損値の確認",
    "href": "posts/2025-04-16-basic-stats-na-check.html#欠損値の確認",
    "title": "基礎統計量と欠損値の確認",
    "section": "欠損値の確認",
    "text": "欠損値の確認\n実際のデータはこんなに綺麗ではないので、しっかりみないとダメですね。 その第一歩として欠損値地の確認を行います。\nまずはデータフレーム全体で欠損値があるかを確認します。\n\nany(is.na(iris))\n\n[1] FALSE\n\n\n結果はTure/FALSEの真偽値で帰ってきます。 Tureが欠損値あり、FALSEが欠損値なしです。 irisには欠損データがないみたいですね。\nせっかくなので、欠損値があるデータを見てみましょう。 欠損値がある組み込みデータとして、ニューヨークの大気汚染と気象データ(airquality)を使用してみます。\n\n# データ全体に欠損があるか？\nany(is.na(airquality))\n\n[1] TRUE\n\n\nこのデータには欠損値があるので、TRUEが帰ってきました。\n次は実際の欠損の個数を確認しましょう。 このコードは２つのことを実行しています。\n\nis.na(airquality) 　→ TRUE / FALSE の 同じ形の論理値データフレームを返します（NA なら TRUE）\nsum(…) 　→ TRUE を1、FALSE を0として合計する 　　→ NA の個数の合計を算出\n\n\n# 欠損の個数\nsum(is.na(airquality))\n\n[1] 44\n\n\nこのデータには44個欠損値がありました。\nどの列(項目)に欠損があるのか、またどのくらいの行に欠損値があるのかを算出します。\n\n# 各列ごとの欠損数\ncolSums(is.na(airquality))\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\n# 欠損値が１つ以上ある行を算出\nsum(!complete.cases(airquality))\n\n[1] 42\n\n\n欠損値が特定の行に集中しているようであれば、行丸ごと集計から外すこともできます。 また、特定の列(項目)に欠損値が多い場合などは、その後の分析に影響してきます。\n心理学の質問紙調査などでは、ある程度綺麗なデータになりますが、ビジネスなどの場面では整形されていないデータで、データのクリーニングから行うことも多いので、その場合はこのような処理が大切になります。\n次回から分析に進んでいこうと思います。"
  },
  {
    "objectID": "posts/2025-4-24-aov1.html",
    "href": "posts/2025-4-24-aov1.html",
    "title": "分散分析 第１回：一元配置分散分析",
    "section": "",
    "text": "今回から何回かに分けて分散分析をやっていきます。 分散分析はいろいろなパターンがあったりするんで、順番にやっていこうと思います。 まずは、分散分析にどんなパターンがあるのか、見てみましょう。\n\n表1：分散分析の種類\n\n\n\n\n\n\n\n\n\n\n分析タイプ\n群の構造\n対象の関係性\n例\nRでのモデル式\n\n\n\n\n一元配置分散分析\n1因子×複数水準\n各群は独立\n3つの異なる学校の成績比較\naov(成績 ~ 学校, data = df)\n\n\n二元配置分散分析\n2因子×各水準\n各群は独立\n性別（男女）×教材（3種）\naov(成績 ~ 性別 * 教材, data = df)\n\n\n繰り返し測定分散分析\n1因子×複数水準\n同一対象に複数条件\n同じ人に3つの音楽を聞かせる\naov(反応 ~ 音楽 + Error(被験者/音楽), data = df)\n\n\n\n\n\n表2：各分析での注意点\n\n\n\n\n\n\n\n\n\n\n分析タイプ\n独立性\n因子数\n前提条件\nよく使うデータ例\n\n\n\n\n一元配置分散分析\n独立\n1\n正規性、等分散、独立\nPlantGrowth\n\n\n二元配置分散分析\n独立\n2\n正規性、等分散、独立、交互作用の考慮\nToothGrowth（2因子）\n\n\n繰り返し測定分散分析\n非独立\n1（対応あり）\n正規性、球面性、繰り返し設計\nsleepstudy, ez::ANT\n\n\n\n\n一元配置分散分析では、1つの独立変数（因子）で群を分け、それらの平均値の違いを検定しています。\n二元配置分散分析では、2つの因子を組み合わせ、主効果と交互作用を検定を行っています。\n繰り返し測定分散分析では、同一被験者に複数の条件を適用したときの条件間差を検定（個人差をコントロール）を行っています。\n\n\n早速、一元配置分散分析をやってみましょう。 今回使用するサンプルデータはPlantGrowth です。\n\n\nPlantGrowth は、植物の成長を示す重さ（weight）を、3つの処理群（ctrl, trt1, trt2）で比較した実験データです。\n\n# データを読み込む\ndata(PlantGrowth)\n\n# データの先頭を確認\nhead(PlantGrowth)\n\n  weight group\n1   4.17  ctrl\n2   5.58  ctrl\n3   5.18  ctrl\n4   6.11  ctrl\n5   4.50  ctrl\n6   4.61  ctrl\n\n# グループごとの観測数を確認\ntable(PlantGrowth$group)\n\n\nctrl trt1 trt2 \n  10   10   10 \n\n# サマリーの確認\nsummary(PlantGrowth)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\n\n一通りデータの確認ができたので、前提条件となる分析をしていきます。\n\n\n\n\n\n\n各グループが正規分布に従っているかを検定・視覚化で確認します。 以前、t検定をする前に でも使用した、シャピロ・ウィルク検定を行います。\n\n# グループごとにシャピロ・ウィルク検定\nby(PlantGrowth$weight, PlantGrowth$group, shapiro.test)\n\nPlantGrowth$group: ctrl\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.95668, p-value = 0.7475\n\n------------------------------------------------------------ \nPlantGrowth$group: trt1\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.93041, p-value = 0.4519\n\n------------------------------------------------------------ \nPlantGrowth$group: trt2\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.94101, p-value = 0.5643\n\n\n結果の読み方の詳細は　t検定をする前に　を参照してください。 今回は全て正規分布していると判断していいでしょう。\n今回は正規分布を視覚的にも確認できるように、図を作成します。 正規分布を確認するための図としては、Q-Qプロットがあります。\n\n\n\nlibrary(ggplot2)\n\nggplot(PlantGrowth, aes(sample = weight)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~ group) +\n  theme_minimal(base_family = \"Hiragino Sans\") +  # 日本語フォント指定\n  labs(title = \"Q-Qプロットによる正規性の確認\")\n\n\n\n\n\n\n\n\n基本的には、正規分布であればデータ点は直線（対角線）上に並びます。 今回はすでにシャピロ・ウィルク検定をしていて、正規分布であることを確認していますが、視覚的にみるとこんな感じです。\n\n\n\n\n続きまして、各グループの分散が等しいことを確認します。 ここではバートレット検定を使用します。\n\nbartlett.test(weight ~ group, data = PlantGrowth)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  weight by group\nBartlett's K-squared = 2.8786, df = 2, p-value = 0.2371\n\n\nさて、結果を読んでいきましょう。\nBartlett test of homogeneity of variances\n等分散性の検定（Bartlett’s test）」を実行したという見出しです。\ndata:  weight by group\n検定に使われたデータの形式を示しています。\n\nweight：目的変数（数値データ）\ngroup：グループ（カテゴリ変数）\n\nBartlett's K-squared = 2.8786, df = 2, p-value = 0.2371\n\nK-squared : Bartlett検定で使われる統計量（大きいほど分散に差がある）\ndf : 自由度（groupの数 - 1）\np-value : 有意確率（等分散であるという仮説が棄却されるかどうかの基準）\n\np値が0.05より大きい（ここでは0.2371）ので、「分散は等しい」という帰無仮説を棄却できないので、”等分散性あり”と判断できます。\n\n\n\n\n正規性も等分散性も問題ないことが確認できたので、実際に分散分析に移っていきましょう。\n\n# 一元配置分散分析\naov_result &lt;- aov(weight ~ group, data = PlantGrowth)\n\n# 結果の表示\nsummary(aov_result)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ngroup        2  3.766  1.8832   4.846 0.0159 *\nResiduals   27 10.492  0.3886                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nこの結果の読み方を見てみましょう。\n\n表3：分散分析の結果の見方\n\n\n\n項目\n意味\n\n\n\n\nDf\n自由度（Degrees of Freedom）\n\n\nSum Sq\n平方和（Sum of Squares）\n\n\nMean Sq\n平均平方（Sum Sq ÷ Df）\n\n\nF value\nF統計量（群間の変動 ÷ 群内の変動）\n\n\nPr(&gt;F)\np値（有意確率）\n\n\n\n\n解説：\ngroup：要因（今回は3群：ctrl, trt1, trt2）\nDf = 2：グループ数 - 1（3 - 1 = 2）\nSum Sq = 3.766：グループ間の平方和（群間のばらつき）\nMean Sq = 1.8832：群間の平方平均（3.766 ÷ 2）\nF value = 4.846：群間の変動が群内の変動より約4.8倍大きいことを示す\nPr(&gt;F) = 0.0159：p値（5%より小さい → 有意）\n*：p値が0.05未満 → 有意な差あり\nこの結果から、「3群のうち、少なくとも1つの平均に有意な差がある」と判断できます（p = 0.0159）。\n\n\n分散分析の結果が出ましたが、ついでに分散分析っぽい感じの図も作成しましょう。 まずは、データの分布を見るために箱ひげ図を作成します。\n\nlibrary(ggplot2)\n\nggplot(PlantGrowth, aes(x = group, y = weight, fill = group)) +\n  geom_boxplot() +\n  theme_minimal(base_family = \"Hiragino Sans\") +  # 日本語フォント指定\n  labs(title = \"各群の重さの分布（箱ひげ図）\",\n       x = \"処理群\", y = \"植物の重さ\")\n\n\n\n\n\n\n\n\nついでに、平均＋標準誤差の棒グラフも作成しましょう。\n\nggplot(PlantGrowth, aes(x = group, y = weight, fill = group)) +\n  stat_summary(fun = mean, geom = \"bar\", width = 0.6) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  theme_minimal(base_family = \"Hiragino Sans\") +  # 日本語フォント指定\n  labs(title = \"各群の平均と標準誤差\",\n       x = \"処理群\", y = \"植物の重さ\")\n\n\n\n\n\n\n\n\n何となくどこに差がありそうか見えてきましたね。\n\n\n\n\n先ほどの分散分析で「3群のうち、少なくとも1つの平均に有意な差がある」ということが分かりました。 次に確認するべきは、「どのグループとどのグループに差があるか」です。 これが多重比較ってやつです。\n多重比較の方法はいろいろありますが、ここでは３群以上の比較でよく使用されるTukeyHSDをやってみます。\n\n# ANOVAの結果オブジェクト\nmodel &lt;- aov(weight ~ group, data = PlantGrowth)\n\n# 多重比較（Tukey HSD）\nTukeyHSD(model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ group, data = PlantGrowth)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nTukeyHSD() 関数の出力結果を一行ずつ見ていきましょう。\nTukey multiple comparisons of means\n95% family-wise confidence level\n解説：\n\n「TukeyのHSD検定」を使って、各グループの平均の差を検定しています。\nfamily-wise confidence level = 95% とは、「すべてのペアの比較に対して、誤検出のリスク（第1種の誤り）を全体で5%以内に保つようにした」信頼区間を意味します。\n\nFit: aov(formula = weight ~ group, data = PlantGrowth)\n解説：\n\n分散分析に使われたモデルを再確認しています。\nweight ~ group：植物の重さを、group（処理群）によって説明するモデル。\n\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\n解説：\n\n\n\n項目\n値\n意味\n\n\n\n\n比較\ntrt1 - ctrl\ntrt1群とctrl群の差\n\n\ndiff\n-0.371\n平均値の差（trt1の方がやや小さい）\n\n\nlwr\n-1.062\n信頼区間の下限（差の下限95%）\n\n\nupr\n0.320\n信頼区間の上限（差の上限95%）\n\n\np adj\n0.391\n補正済みp値（有意ではない）\n\n\n\n解釈：trt1とctrlの間には有意な差は見られない（p &gt; 0.05、信頼区間に0が含まれている）。\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\n解説：\n\n平均値の差は +0.494 → trt2の方が重い。\nただし信頼区間（-0.197 ～ 1.185）に0が含まれており、p = 0.198 &gt; 0.05。\n\n解釈：trt2とctrlの間も有意な差はなし。\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n解説：\n\n平均差は 0.865 → trt2の方がtrt1より有意に重い。\n信頼区間（0.174 ～ 1.556）には0を含まない。\np値 = 0.012 → 有意水準0.05未満。\n\n解釈：trt2 と trt1 の間には 統計的に有意な差がある。\nTukeyの多重比較の結果、次のようなことが分かりました。\n\ntrt1とctrl、trt2とctrlの間には統計的に有意な差は見られませんでした（p &gt; 0.05、信頼区間に0を含む）。\n一方で、trt2とtrt1の間には有意な差がありました（p = 0.012）。\n→ trt2群の植物は、trt1群よりも有意に重い\n\nこの結果を図にしてみましょう。\n\n\n\n# モデル作成\nmodel &lt;- aov(weight ~ group, data = PlantGrowth)\n\n# 多重比較（Tukey）\ntukey_result &lt;- TukeyHSD(model)\n\n# プロット（標準の可視化）\nplot(tukey_result, las = 1, col = \"blue\")\n\n\n\n\n\n\n\n\nggplot2でもっと見やすくしましょう。\n\n# 1. モデル作成とTukeyHSD\nmodel &lt;- aov(weight ~ group, data = PlantGrowth)\ntukey &lt;- TukeyHSD(model)\n\n# 2. データフレーム化（グループ情報も列にする）\ntukey_df &lt;- as.data.frame(tukey$group)\ntukey_df$comparison &lt;- rownames(tukey$group)\n\n# 3. 可視化\nlibrary(ggplot2)\n\nggplot(tukey_df, aes(x = comparison, y = diff)) +\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.2, color = \"steelblue\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  coord_flip() +\n  labs(title = \"Tukey HSDの多重比較結果（平均差と信頼区間）\",\n       x = \"比較グループ\", y = \"平均差\") +\n  theme_minimal(base_family = \"Hiragino Sans\")  # 日本語フォント指定\n\n\n\n\n\n\n\n\n今回の分析結果を可視化できたので、結構分かりやすいのではないかなと思います。 次回は、二元配置分散分析をやってみたいと思います。"
  },
  {
    "objectID": "posts/2025-4-24-aov1.html#一元配置分散分析",
    "href": "posts/2025-4-24-aov1.html#一元配置分散分析",
    "title": "分散分析 第１回：一元配置分散分析",
    "section": "",
    "text": "早速、一元配置分散分析をやってみましょう。 今回使用するサンプルデータはPlantGrowth です。\n\n\nPlantGrowth は、植物の成長を示す重さ（weight）を、3つの処理群（ctrl, trt1, trt2）で比較した実験データです。\n\n# データを読み込む\ndata(PlantGrowth)\n\n# データの先頭を確認\nhead(PlantGrowth)\n\n  weight group\n1   4.17  ctrl\n2   5.58  ctrl\n3   5.18  ctrl\n4   6.11  ctrl\n5   4.50  ctrl\n6   4.61  ctrl\n\n# グループごとの観測数を確認\ntable(PlantGrowth$group)\n\n\nctrl trt1 trt2 \n  10   10   10 \n\n# サマリーの確認\nsummary(PlantGrowth)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\n\n一通りデータの確認ができたので、前提条件となる分析をしていきます。"
  },
  {
    "objectID": "posts/2025-4-24-aov1.html#分散分析の前提条件チェック",
    "href": "posts/2025-4-24-aov1.html#分散分析の前提条件チェック",
    "title": "分散分析 第１回：一元配置分散分析",
    "section": "",
    "text": "各グループが正規分布に従っているかを検定・視覚化で確認します。 以前、t検定をする前に でも使用した、シャピロ・ウィルク検定を行います。\n\n# グループごとにシャピロ・ウィルク検定\nby(PlantGrowth$weight, PlantGrowth$group, shapiro.test)\n\nPlantGrowth$group: ctrl\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.95668, p-value = 0.7475\n\n------------------------------------------------------------ \nPlantGrowth$group: trt1\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.93041, p-value = 0.4519\n\n------------------------------------------------------------ \nPlantGrowth$group: trt2\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.94101, p-value = 0.5643\n\n\n結果の読み方の詳細は　t検定をする前に　を参照してください。 今回は全て正規分布していると判断していいでしょう。\n今回は正規分布を視覚的にも確認できるように、図を作成します。 正規分布を確認するための図としては、Q-Qプロットがあります。\n\n\n\nlibrary(ggplot2)\n\nggplot(PlantGrowth, aes(sample = weight)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~ group) +\n  theme_minimal(base_family = \"Hiragino Sans\") +  # 日本語フォント指定\n  labs(title = \"Q-Qプロットによる正規性の確認\")\n\n\n\n\n\n\n\n\n基本的には、正規分布であればデータ点は直線（対角線）上に並びます。 今回はすでにシャピロ・ウィルク検定をしていて、正規分布であることを確認していますが、視覚的にみるとこんな感じです。\n\n\n\n\n続きまして、各グループの分散が等しいことを確認します。 ここではバートレット検定を使用します。\n\nbartlett.test(weight ~ group, data = PlantGrowth)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  weight by group\nBartlett's K-squared = 2.8786, df = 2, p-value = 0.2371\n\n\nさて、結果を読んでいきましょう。\nBartlett test of homogeneity of variances\n等分散性の検定（Bartlett’s test）」を実行したという見出しです。\ndata:  weight by group\n検定に使われたデータの形式を示しています。\n\nweight：目的変数（数値データ）\ngroup：グループ（カテゴリ変数）\n\nBartlett's K-squared = 2.8786, df = 2, p-value = 0.2371\n\nK-squared : Bartlett検定で使われる統計量（大きいほど分散に差がある）\ndf : 自由度（groupの数 - 1）\np-value : 有意確率（等分散であるという仮説が棄却されるかどうかの基準）\n\np値が0.05より大きい（ここでは0.2371）ので、「分散は等しい」という帰無仮説を棄却できないので、”等分散性あり”と判断できます。"
  },
  {
    "objectID": "posts/2025-4-24-aov1.html#分散分析-1",
    "href": "posts/2025-4-24-aov1.html#分散分析-1",
    "title": "分散分析 第１回：一元配置分散分析",
    "section": "",
    "text": "正規性も等分散性も問題ないことが確認できたので、実際に分散分析に移っていきましょう。\n\n# 一元配置分散分析\naov_result &lt;- aov(weight ~ group, data = PlantGrowth)\n\n# 結果の表示\nsummary(aov_result)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ngroup        2  3.766  1.8832   4.846 0.0159 *\nResiduals   27 10.492  0.3886                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nこの結果の読み方を見てみましょう。\n\n表3：分散分析の結果の見方\n\n\n\n項目\n意味\n\n\n\n\nDf\n自由度（Degrees of Freedom）\n\n\nSum Sq\n平方和（Sum of Squares）\n\n\nMean Sq\n平均平方（Sum Sq ÷ Df）\n\n\nF value\nF統計量（群間の変動 ÷ 群内の変動）\n\n\nPr(&gt;F)\np値（有意確率）\n\n\n\n\n解説：\ngroup：要因（今回は3群：ctrl, trt1, trt2）\nDf = 2：グループ数 - 1（3 - 1 = 2）\nSum Sq = 3.766：グループ間の平方和（群間のばらつき）\nMean Sq = 1.8832：群間の平方平均（3.766 ÷ 2）\nF value = 4.846：群間の変動が群内の変動より約4.8倍大きいことを示す\nPr(&gt;F) = 0.0159：p値（5%より小さい → 有意）\n*：p値が0.05未満 → 有意な差あり\nこの結果から、「3群のうち、少なくとも1つの平均に有意な差がある」と判断できます（p = 0.0159）。\n\n\n分散分析の結果が出ましたが、ついでに分散分析っぽい感じの図も作成しましょう。 まずは、データの分布を見るために箱ひげ図を作成します。\n\nlibrary(ggplot2)\n\nggplot(PlantGrowth, aes(x = group, y = weight, fill = group)) +\n  geom_boxplot() +\n  theme_minimal(base_family = \"Hiragino Sans\") +  # 日本語フォント指定\n  labs(title = \"各群の重さの分布（箱ひげ図）\",\n       x = \"処理群\", y = \"植物の重さ\")\n\n\n\n\n\n\n\n\nついでに、平均＋標準誤差の棒グラフも作成しましょう。\n\nggplot(PlantGrowth, aes(x = group, y = weight, fill = group)) +\n  stat_summary(fun = mean, geom = \"bar\", width = 0.6) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  theme_minimal(base_family = \"Hiragino Sans\") +  # 日本語フォント指定\n  labs(title = \"各群の平均と標準誤差\",\n       x = \"処理群\", y = \"植物の重さ\")\n\n\n\n\n\n\n\n\n何となくどこに差がありそうか見えてきましたね。"
  },
  {
    "objectID": "posts/2025-4-24-aov1.html#多重比較",
    "href": "posts/2025-4-24-aov1.html#多重比較",
    "title": "分散分析 第１回：一元配置分散分析",
    "section": "",
    "text": "先ほどの分散分析で「3群のうち、少なくとも1つの平均に有意な差がある」ということが分かりました。 次に確認するべきは、「どのグループとどのグループに差があるか」です。 これが多重比較ってやつです。\n多重比較の方法はいろいろありますが、ここでは３群以上の比較でよく使用されるTukeyHSDをやってみます。\n\n# ANOVAの結果オブジェクト\nmodel &lt;- aov(weight ~ group, data = PlantGrowth)\n\n# 多重比較（Tukey HSD）\nTukeyHSD(model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ group, data = PlantGrowth)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nTukeyHSD() 関数の出力結果を一行ずつ見ていきましょう。\nTukey multiple comparisons of means\n95% family-wise confidence level\n解説：\n\n「TukeyのHSD検定」を使って、各グループの平均の差を検定しています。\nfamily-wise confidence level = 95% とは、「すべてのペアの比較に対して、誤検出のリスク（第1種の誤り）を全体で5%以内に保つようにした」信頼区間を意味します。\n\nFit: aov(formula = weight ~ group, data = PlantGrowth)\n解説：\n\n分散分析に使われたモデルを再確認しています。\nweight ~ group：植物の重さを、group（処理群）によって説明するモデル。\n\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\n解説：\n\n\n\n項目\n値\n意味\n\n\n\n\n比較\ntrt1 - ctrl\ntrt1群とctrl群の差\n\n\ndiff\n-0.371\n平均値の差（trt1の方がやや小さい）\n\n\nlwr\n-1.062\n信頼区間の下限（差の下限95%）\n\n\nupr\n0.320\n信頼区間の上限（差の上限95%）\n\n\np adj\n0.391\n補正済みp値（有意ではない）\n\n\n\n解釈：trt1とctrlの間には有意な差は見られない（p &gt; 0.05、信頼区間に0が含まれている）。\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\n解説：\n\n平均値の差は +0.494 → trt2の方が重い。\nただし信頼区間（-0.197 ～ 1.185）に0が含まれており、p = 0.198 &gt; 0.05。\n\n解釈：trt2とctrlの間も有意な差はなし。\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n解説：\n\n平均差は 0.865 → trt2の方がtrt1より有意に重い。\n信頼区間（0.174 ～ 1.556）には0を含まない。\np値 = 0.012 → 有意水準0.05未満。\n\n解釈：trt2 と trt1 の間には 統計的に有意な差がある。\nTukeyの多重比較の結果、次のようなことが分かりました。\n\ntrt1とctrl、trt2とctrlの間には統計的に有意な差は見られませんでした（p &gt; 0.05、信頼区間に0を含む）。\n一方で、trt2とtrt1の間には有意な差がありました（p = 0.012）。\n→ trt2群の植物は、trt1群よりも有意に重い\n\nこの結果を図にしてみましょう。\n\n\n\n# モデル作成\nmodel &lt;- aov(weight ~ group, data = PlantGrowth)\n\n# 多重比較（Tukey）\ntukey_result &lt;- TukeyHSD(model)\n\n# プロット（標準の可視化）\nplot(tukey_result, las = 1, col = \"blue\")\n\n\n\n\n\n\n\n\nggplot2でもっと見やすくしましょう。\n\n# 1. モデル作成とTukeyHSD\nmodel &lt;- aov(weight ~ group, data = PlantGrowth)\ntukey &lt;- TukeyHSD(model)\n\n# 2. データフレーム化（グループ情報も列にする）\ntukey_df &lt;- as.data.frame(tukey$group)\ntukey_df$comparison &lt;- rownames(tukey$group)\n\n# 3. 可視化\nlibrary(ggplot2)\n\nggplot(tukey_df, aes(x = comparison, y = diff)) +\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.2, color = \"steelblue\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  coord_flip() +\n  labs(title = \"Tukey HSDの多重比較結果（平均差と信頼区間）\",\n       x = \"比較グループ\", y = \"平均差\") +\n  theme_minimal(base_family = \"Hiragino Sans\")  # 日本語フォント指定\n\n\n\n\n\n\n\n\n今回の分析結果を可視化できたので、結構分かりやすいのではないかなと思います。 次回は、二元配置分散分析をやってみたいと思います。"
  },
  {
    "objectID": "posts/2025-4-17-t-test.html",
    "href": "posts/2025-4-17-t-test.html",
    "title": "t検定",
    "section": "",
    "text": "今回はt検定を行なっていきます。 使用する組み込みデータは下記の通りです。\n対応のないt検定(独立2郡の比較) 組み込みデータ；ToothGrowth 概要；サプリメントの種類と用量による歯の成長比較\n対応のあるt検定(同一人物) 組み込みデータ；sleep 概要；薬剤AとBを同一被験者に使った反応の比較"
  },
  {
    "objectID": "posts/2025-4-17-t-test.html#今回のお題",
    "href": "posts/2025-4-17-t-test.html#今回のお題",
    "title": "t検定",
    "section": "",
    "text": "今回はt検定を行なっていきます。 使用する組み込みデータは下記の通りです。\n対応のないt検定(独立2郡の比較) 組み込みデータ；ToothGrowth 概要；サプリメントの種類と用量による歯の成長比較\n対応のあるt検定(同一人物) 組み込みデータ；sleep 概要；薬剤AとBを同一被験者に使った反応の比較"
  },
  {
    "objectID": "posts/2025-4-17-t-test.html#対応のないt検定",
    "href": "posts/2025-4-17-t-test.html#対応のないt検定",
    "title": "t検定",
    "section": "対応のないt検定",
    "text": "対応のないt検定\nまずは対応のないt検定です。 今回使用するデータの中身を見ていきましょう。\n\nデータの確認\n\ndata(ToothGrowth)\nstr(ToothGrowth)\n\n'data.frame':   60 obs. of  3 variables:\n $ len : num  4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ...\n $ supp: Factor w/ 2 levels \"OJ\",\"VC\": 2 2 2 2 2 2 2 2 2 2 ...\n $ dose: num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n\nsummary(ToothGrowth)\n\n      len        supp         dose      \n Min.   : 4.20   OJ:30   Min.   :0.500  \n 1st Qu.:13.07   VC:30   1st Qu.:0.500  \n Median :19.25           Median :1.000  \n Mean   :18.81           Mean   :1.167  \n 3rd Qu.:25.27           3rd Qu.:2.000  \n Max.   :33.90           Max.   :2.000  \n\n\n３つの項目があり、それぞれ下記のようになっています。\n\nlen : 歯の成長量\nsupp : サプリメントの種類(2種OJ = オレンジジュース, VC = ビタミンC)\ndose : サプリメントの摂取量\n\n早速、t検定をしていきましょう。 t検定の関数は簡単です。\nt.test(“従属変数名” ~ “独立変数名”, data = “データ名”)\n実際にやってみましょう。\n\nt.test(len ~ supp, data = ToothGrowth)\n\n\n    Welch Two Sample t-test\n\ndata:  len by supp\nt = 1.9153, df = 55.309, p-value = 0.06063\nalternative hypothesis: true difference in means between group OJ and group VC is not equal to 0\n95 percent confidence interval:\n -0.1710156  7.5710156\nsample estimates:\nmean in group OJ mean in group VC \n        20.66333         16.96333 \n\n\n無事結果が出力されたので読んでいきましょう。\nWelch Two Sample t-test\nウェルチの２郡間でのt検定が実施されたことが書かれています。\ndata:  len by supp\n比較対象は len（歯の長さ）,グループ分け変数は supp（OJ vs VC）で分析が行われたことが記載されています。\nt = 1.9153, df = 55.309, p-value = 0.06063\nt -&gt; t値（2群の平均差を標準誤差で割った値） df -&gt; 自由度（Welchの調整による） p-value -&gt; 有意確率：\nalternative hypothesis: true difference in means between group OJ and group VC is not equal to 0\n仮説を書いてくれています。 「対立仮説：OJ群とVC群の平均値の真の差は0に等しくない」by DeepL\n95 percent confidence interval:\n -0.1710156  7.5710156\n95%信頼区間が書かれています。 「2群の平均差は -0.17 〜 +7.57 の範囲にあると考えられる」ということです。\nsample estimates:\nmean in group OJ mean in group VC \n        20.66333         16.96333 \nサンプルの２群のそれぞれの平均値です。\n１つのコマンドでかなり多くのことを実施して出力してくれたことが分かります。 これを手で計算していた時代があると思うと信じられないですね。\n今回は有意確率(p-value)が0.06063 なので5%水準で見ても有意ではありませんでした。"
  },
  {
    "objectID": "posts/2025-4-17-t-test.html#対応のあるt検定",
    "href": "posts/2025-4-17-t-test.html#対応のあるt検定",
    "title": "t検定",
    "section": "対応のあるt検定",
    "text": "対応のあるt検定\n続いては、Rの組み込みデータ sleep を使って、対応のあるt検定の流れを見ていきます。\n\nデータの確認\nまずは使用するデータの中身を確認しましょう。\n\ndata(sleep)\nstr(sleep)\n\n'data.frame':   20 obs. of  3 variables:\n $ extra: num  0.7 -1.6 -0.2 -1.2 -0.1 3.4 3.7 0.8 0 2 ...\n $ group: Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n $ ID   : Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n\nsummary(sleep)\n\n     extra        group        ID   \n Min.   :-1.600   1:10   1      :2  \n 1st Qu.:-0.025   2:10   2      :2  \n Median : 0.950          3      :2  \n Mean   : 1.540          4      :2  \n 3rd Qu.: 3.400          5      :2  \n Max.   : 5.500          6      :2  \n                         (Other):8  \n\nhead(sleep)\n\n  extra group ID\n1   0.7     1  1\n2  -1.6     1  2\n3  -0.2     1  3\n4  -1.2     1  4\n5  -0.1     1  5\n6   3.4     1  6\n\n\nこのデータには以下の3列があります：\n\nextra : 睡眠時間の増加量（数値）\ngroup : 投与された薬剤の種類（1または2）\nID : 被験者ID（同じ被験者が両方の薬剤を試している）\n\nつまり、このデータは同一被験者に2種類の薬を与えた前後比較になっており、対応のあるt検定（paired t-test）が適用できます。\n\n\n\nデータの変換（wide形式へ）\nt.test() で対応のあるt検定をするには、2つのベクトルが必要です。 まずは group によって分けて、同じIDごとに列に並べ直します。\n\nsleep_wide &lt;- reshape(sleep, timevar = \"group\", idvar = \"ID\", direction = \"wide\")\nhead(sleep_wide)\n\n  ID extra.1 extra.2\n1  1     0.7     1.9\n2  2    -1.6     0.8\n3  3    -0.2     1.1\n4  4    -1.2     0.1\n5  5    -0.1    -0.1\n6  6     3.4     4.4\n\n\nこのようにすると extra.1（薬1）と extra.2（薬2）が並んだデータになります。 分析によっては元のデータを加工して、別のデータフレームを作成して分析を行なっていくこよはよくあります。\n\n\n\nt検定の実行\nそれでは対応のあるt検定を実行してみましょう。\n\nt.test(sleep_wide$extra.1, sleep_wide$extra.2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  sleep_wide$extra.1 and sleep_wide$extra.2\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\n\n\n\n出力の読み取り\nPaired t-test\n対応のあるt検定（paired t-test）が実行されたことを示しています。\ndata:  sleep_wide$extra.1 and sleep_wide$extra.2\n比較したのは薬1と薬2による睡眠時間の増加量です。\nt = -4.0621, df = 9, p-value = 0.002833\n\nt はt値：2群の平均差を標準誤差で割った値。\ndf は自由度（今回は10人なので df = 9）。\np-value は有意確率：0.05より十分小さいので、有意差ありと判断できます。\n\nalternative hypothesis: true mean difference is not equal to 0\n対立仮説は「2群の平均に差がある（= 差が0でない）」です。\n95 percent confidence interval:\n -2.459885 -0.700115\n薬剤1と薬剤2の平均差は、95%信頼区間で -2.46 〜 -0.70。 0を含まないため、有意差あり。\nsample estimates:\nmean of the differences\n                 -1.58\n薬剤2の方が平均して 1.58 時間分 睡眠時間が長くなる傾向があります。\n\nこちらは有意差が出てよかったです。 私も記事を書きながら分析をしているので、両方とも有意差なしだと悲しいところでした。\n\n\nまとめ\nさて今回は、t検定のやり方と結果の見方を紹介しました。 ここでは書いていませんが、t検定前にやるべきことがありました。 それは別記事で書こうと思うので、t検定をやる際にはそちらもご参照ください。"
  },
  {
    "objectID": "posts/2025-04-14-r-blog-start.html",
    "href": "posts/2025-04-14-r-blog-start.html",
    "title": "Rの再学習、はじめました",
    "section": "",
    "text": "およそ8年ぶりにRを使って再学習します。\n大学を卒業して以来、ちょくちょくRに触れるようにしてはいましたが、だんだんと仕事で使う他の言語がメインになっていき、かれこれ8年ほど経ってしました。\n社会人になると「やりたい」と思っていても、なかなか時間を作るのが難しいものです。\nそんな折に、学生時代の友人から「Rの使い方を教えて欲しい」との連絡がありました。 大学院など進学して研究を続けている友人はすごいなと思いました。私も研究の道の興味がなかったわけではなかったので。\nそんなきっかけで久しぶりにRに触れてみることになりました。 目的が明確にあったり、他人が絡んでいると人間不思議と体が動くものです。\n細かい話は忘れてしまっていますが、全体像や分析の方針のことを意外と覚えていたりしました。 大学時代を懐かしく思いながら、昔自分の備忘録的に書いたブログを思い出しました。 他の人に読んでもらう意欲の低すぎるブログで恥ずかしくなったので、せっかくの機会だとRと統計の再学習を兼ねてブログを書くことにしました。\nプログラミング言語としてはRはメジャーではないかもしれませんが、大学で統計などのために学習する人もいるのではないかと思います。 そんな人たちの参考になればいいなと思っています。\n気楽に書いていくので、気楽に読んでください。\nそしてRをきっかけに他のプログラミング言語にも興味を持っていただけると嬉しいです。"
  },
  {
    "objectID": "posts/2025-5-1-aov3.html",
    "href": "posts/2025-5-1-aov3.html",
    "title": "分散分析 第3回：繰り返しのある分散分析(反復測定分散分析)",
    "section": "",
    "text": "今回は繰り返しのある分散分析(反復測定分散分析)をやっていきます。 早速今回使用するデータを見ながら、 「そもそも”繰り返しのある”ってなによ」というところから、 確認してみましょう。\n\n\n今回使用するデータはChickWeightです。 このブログで取り上げるのは初めてになると思います。\n\ndata(ChickWeight)\nsummary(ChickWeight)\n\n     weight           Time           Chick     Diet   \n Min.   : 35.0   Min.   : 0.00   13     : 12   1:220  \n 1st Qu.: 63.0   1st Qu.: 4.00   9      : 12   2:120  \n Median :103.0   Median :10.00   20     : 12   3:120  \n Mean   :121.8   Mean   :10.72   10     : 12   4:118  \n 3rd Qu.:163.8   3rd Qu.:16.00   17     : 12          \n Max.   :373.0   Max.   :21.00   19     : 12          \n                                 (Other):506          \n\n\n\nweight : 測定されたヒヨコの体重(数値)\nTime : 体重が測定された日数(数値)\nChick : ヒヨコの個体番号(名義)\nDiet : ヒヨコに与えられた餌の種類(因子)\n\nt検定での”繰り返しのある”という表現がありましたが、同一の調査対象に対する分析において使われることでした。 今回の例でも、ヒヨコの成長の記録のデータの分析に”繰り返しのある”分散分析を行うわけです。\n最もよく使う場面が、事前(pre)・事後(pos)の比較ではないでしょうか。\n体重が測定された日数がどうなっているかも見てみましょう。\n\nunique(ChickWeight$Time)\n\n [1]  0  2  4  6  8 10 12 14 16 18 20 21\n\n\n1日おきに計測している感じですね。 最後だけは連続ですが。\n\n\n\nChickWeight のデータを分散分析をしていきます。\n独立変数は、Time と Diet で 従属変数が、weightです。\nつまり、繰り返しのある二元配置分散分析ということになります。 与える餌の種類によるヒヨコの体重増加の違いを仮定した分析になります。\n一気に分散分析をしても良いですが、データの可視化をして結果を想像しながら進められたら良いなと思います。\n\nデータの可視化\n前処理\n繰り返しのある分散分析\n結果の可視化\n\n\n\n\n実際に分析を進めていきましょう。\n\n\nまずは餌の種類ごとに色分けしたヒヨコの体重増加を線グラフに表してみましょう。\n\nlibrary(ggplot2)\n\nggplot(ChickWeight, aes(x = Time, y = weight, group = Chick, color = Diet)) +\n  geom_line(alpha = 0.5) +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"各ヒヨコの体重変化（Diet別）\",\n       x = \"日数（Time）\",\n       y = \"体重（g）\",\n       color = \"餌の種類（Diet）\")\n\n\n\n\n\n\n\n\nなんか見づらいですね。 1匹ずつのグラフなので見辛いのかもしれません。 餌毎の平均線を強調して入れてみましょう。\n\nggplot(ChickWeight, aes(x = Time, y = weight, group = Chick)) +\n  geom_line(alpha = 0.4, color = \"gray\") +\n  stat_summary(aes(group = Diet, color = Diet), fun = mean, geom = \"line\", size = 1.2) +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"各ヒヨコの体重とDietごとの平均曲線\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nこれで大分見やすくなりましたね。 平均値では差がありそうですが、21日目での体重にはかなりの差がありそうです。 また、途中で何かしらの原因(死亡とか？)で21日前に切れているデータもありあそうです。\n\n\n\n前処理として、データの加工を行なっていきます。 まずTimeが現在数値になっており、このままでは使用できないのでfactor(要因)型に変換しましょう。\n\nChickWeight$Time &lt;- as.factor(ChickWeight$Time)\n\nこれで、Timeがfactor(要因)型になりました。 しかし、このままでは12水準あり、多重比較やその後の解釈が大変になりそうです。 そこで今回は、開始ー中間ー最終の３地点のみでの比較にしましょう。\nfactor(要因)型だと処理がうまくいかないようなので、一旦数値型に戻しておきましょう。 この話は後日記事にしようと思います。\n\n# まずTimeを数値に戻しておく（もともとfactorにしていた場合）\nChickWeight$Time &lt;- as.numeric(as.character(ChickWeight$Time))\n\n# 0, 10, 21日目のデータだけ抽出\nchick_subset &lt;- subset(ChickWeight, Time %in% c(0, 10, 21))\n\n# Timeをfactorに変換する\nchick_subset$Time &lt;- as.factor(chick_subset$Time)\n\nいろいろと処理をしたので、現在のデータが意図した形になっているかを確認してみましょう。\nサブセット後の基本構造を確認\n\nstr(chick_subset)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  144 obs. of  4 variables:\n $ weight: num  42 93 205 40 103 215 43 99 202 42 ...\n $ Time  : Factor w/ 3 levels \"0\",\"10\",\"21\": 1 2 3 1 2 3 1 2 3 1 ...\n $ Chick : Ord.factor w/ 50 levels \"18\"&lt;\"16\"&lt;\"15\"&lt;..: 15 15 15 17 17 17 14 14 14 11 ...\n $ Diet  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(chick_subset)\n\n     weight      Time        Chick     Diet  \n Min.   : 39.0   0 :50   13     :  3   1:55  \n 1st Qu.: 42.0   10:49   9      :  3   2:30  \n Median :102.5   21:45   20     :  3   3:30  \n Mean   :119.3           10     :  3   4:29  \n 3rd Qu.:157.2           17     :  3         \n Max.   :373.0           19     :  3         \n                         (Other):126         \n\n\nTime列が factor になっているか\n\nclass(chick_subset$Time)      # \"factor\" になっていればOK\n\n[1] \"factor\"\n\nis.factor(chick_subset$Time)  # TRUE ならOK\n\n[1] TRUE\n\n\nTime の水準を確認\n\nlevels(chick_subset$Time)\n\n[1] \"0\"  \"10\" \"21\"\n\ntable(chick_subset$Time)  # 各水準に何件あるかも同時に確認\n\n\n 0 10 21 \n50 49 45 \n\n\nChick が factor になっているか\n\nis.factor(chick_subset$Chick)\n\n[1] TRUE\n\nlevels(chick_subset$Chick)\n\n [1] \"18\" \"16\" \"15\" \"13\" \"9\"  \"20\" \"10\" \"8\"  \"17\" \"19\" \"4\"  \"6\"  \"11\" \"3\"  \"1\" \n[16] \"12\" \"2\"  \"5\"  \"14\" \"7\"  \"24\" \"30\" \"22\" \"23\" \"27\" \"28\" \"26\" \"25\" \"29\" \"21\"\n[31] \"33\" \"37\" \"36\" \"31\" \"39\" \"38\" \"32\" \"40\" \"34\" \"35\" \"44\" \"45\" \"43\" \"41\" \"47\"\n[46] \"49\" \"46\" \"50\" \"42\" \"48\"\n\n\nさて、一気に確認しましたが、その中で気になることが1つあります。 それは、”Time の水準を確認”をした際に、欠損値が出ていることです。 繰り返しのある分散分析の場合は、データが完全であることが前提なので、欠損値があると分析時にエラーになってしまいます。 選択肢としては、下記があります - 欠損のあるデータを除外する - 欠損補填をする(推定値で補完) - 他の分析にする\nということで、今回は最も一般的な方法である欠損値の除外をしてしまいましょう。\n\n# 各 Chick が Time = 0, 10, 21 をすべて持っているかを確認\nlibrary(dplyr)\n\nWarning: パッケージ 'dplyr' はバージョン 4.2.3 の R の下で造られました\n\n\n\n 次のパッケージを付け加えます: 'dplyr' \n\n\n 以下のオブジェクトは 'package:stats' からマスクされています:\n\n    filter, lag\n\n\n 以下のオブジェクトは 'package:base' からマスクされています:\n\n    intersect, setdiff, setequal, union\n\ncomplete_chicks &lt;- chick_subset %&gt;%\n  group_by(Chick) %&gt;%\n  summarise(n = n()) %&gt;%\n  filter(n == 3) %&gt;%\n  pull(Chick)\n\n# 完全なヒヨコだけを抽出\nchick_complete &lt;- chick_subset %&gt;%\n  filter(Chick %in% complete_chicks) %&gt;%\n  droplevels()\n\nさてデータを確認してみましょう。\n\nlevels(chick_complete$Time)\n\n[1] \"0\"  \"10\" \"21\"\n\ntable(chick_complete$Time)  # 各水準に何件あるかも同時に確認\n\n\n 0 10 21 \n45 45 45 \n\n\nこれで良い感じになりましたね。 では、ここからは正規性と等分散性の確認に入りましょう。\n\n\n\n今回も正規性と等分散性の検定はサクッとです。\n\nlibrary(dplyr)\n\nchick_complete %&gt;%\n  group_by(Time) %&gt;%\n  summarise(p_value = shapiro.test(weight)$p.value)\n\n# A tibble: 3 × 2\n  Time   p_value\n  &lt;fct&gt;    &lt;dbl&gt;\n1 0     0.000752\n2 10    0.465   \n3 21    0.869   \n\n\nTimeが0の場合の正規性に問題がありそうです。 Q-Qプロットで見てみましょう。\n\nlibrary(ggplot2)\n\nggplot(chick_complete %&gt;% filter(Time == 0), aes(sample = weight)) +\n  stat_qq() +\n  stat_qq_line() +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"Time = 0 の Q-Qプロット\",\n       x = \"理論分位点\", y = \"観測分位点\")\n\n\n\n\n\n\n\n\n判断しづらいところですね。 一旦はこのまま進めましょう。\n次は等分散性の確認です。\n\nbartlett.test(weight ~ Time, data = chick_complete)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  weight by Time\nBartlett's K-squared = 330.44, df = 2, p-value &lt; 2.2e-16\n\n\n等分散性に問題ありですね。 これが論文であれば、分散分析をやめて他の分析に変更しないといけなくなりそうです。 線形混合モデルとかになるみたいですが、それはまた次回にしましょう。\n今回は繰り返しのある分散分析のやり方を紹介したいので、この問題には目を瞑りましょう。 すみません。\n\n\n\n問題を抱えつつ分析をしてみましょう。\n繰り返しがある場合は、指定が少し面倒なので一度時間のみの１要因で3水準での分析をしてみましょう。\n\n# Chick ごとに Time に沿って繰り返し測定されている構造を Error(Chick/Time) で指定しています。\naov_result &lt;- aov(weight ~ Time + Error(Chick/Time), data = chick_complete)\nsummary(aov_result)\n\n\nError: Chick\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 44 114242    2596               \n\nError: Chick:Time\n          Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTime       2 721615  360808   238.6 &lt;2e-16 ***\nResiduals 88 133068    1512                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n時間による成長は当然のごとく出ますね。 繰り返し測定をしている要因をErrorを指定しているのがポイントです。\nでは、２要因の場合をやってみましょう。\n\naov2 &lt;- aov(weight ~ Time * Diet + Error(Chick/Time), data = chick_complete)\nsummary(aov2)\n\n\nError: Chick\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet       3  30604   10201   5.001 0.00478 **\nResiduals 41  83637    2040                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Chick:Time\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTime       2 721615  360808 293.492  &lt; 2e-16 ***\nTime:Diet  6  32260    5377   4.374 0.000701 ***\nResiduals 82 100808    1229                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n結果の見方はこれまでの分散分析と変わらないので、詳細は省きます(以前の分散分析の記事を参照)。 結果を要約します。 - 餌の種類の主効果あり - 時間による主効果あり - 餌と時間による交互作用あり\nでした。 ここからは多重比較をしましょう。\n\n\n\n前回やった二要因分散分析では多重比較でTukeyHSD()を使用しましたが、繰り返しのある分散分析はこれが使えません。 繰り返しがあることをオプションで使えないようです。\nそのため、Time × Diet の組み合わせを新しい要因として作成して1要因12水準の多重比較の分析に変えます。\n組み合わせでの新しい要因の作成\n\nchick_complete$group &lt;- interaction(chick_complete$Time, chick_complete$Diet)\ntable(chick_complete$group)\n\n\n 0.1 10.1 21.1  0.2 10.2 21.2  0.3 10.3 21.3  0.4 10.4 21.4 \n  16   16   16   10   10   10   10   10   10    9    9    9 \n\n\nこれで新しい要因は作れたので、多重比較を行いましょう。\n\noneway_model &lt;- aov(weight ~ group, data = chick_complete)\nTukeyHSD(oneway_model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ group, data = chick_complete)\n\n$group\n                   diff         lwr         upr     p adj\n10.1-0.1    55.68750000   10.069381  101.305619 0.0046278\n21.1-0.1   136.18750000   90.569381  181.805619 0.0000000\n0.2-0.1     -0.86250000  -52.875158   51.150158 1.0000000\n10.2-0.1    66.93750000   14.924842  118.950158 0.0020618\n21.2-0.1   173.13750000  121.124842  225.150158 0.0000000\n0.3-0.1     -0.76250000  -52.775158   51.250158 1.0000000\n10.3-0.1    75.53750000   23.524842  127.550158 0.0002339\n21.3-0.1   228.73750000  176.724842  280.750158 0.0000000\n0.4-0.1     -0.67361111  -54.435080   53.087858 1.0000000\n10.4-0.1    85.32638889   31.564920  139.087858 0.0000343\n21.4-0.1   196.99305556  143.231587  250.754525 0.0000000\n21.1-10.1   80.50000000   34.881881  126.118119 0.0000023\n0.2-10.1   -56.55000000 -108.562658   -4.537342 0.0207934\n10.2-10.1   11.25000000  -40.762658   63.262658 0.9998829\n21.2-10.1  117.45000000   65.437342  169.462658 0.0000000\n0.3-10.1   -56.45000000 -108.462658   -4.437342 0.0212194\n10.3-10.1   19.85000000  -32.162658   71.862658 0.9811669\n21.3-10.1  173.05000000  121.037342  225.062658 0.0000000\n0.4-10.1   -56.36111111 -110.122580   -2.599642 0.0310837\n10.4-10.1   29.63888889  -24.122580   83.400358 0.7943625\n21.4-10.1  141.30555556   87.544087  195.067025 0.0000000\n0.2-21.1  -137.05000000 -189.062658  -85.037342 0.0000000\n10.2-21.1  -69.25000000 -121.262658  -17.237342 0.0011729\n21.2-21.1   36.95000000  -15.062658   88.962658 0.4361857\n0.3-21.1  -136.95000000 -188.962658  -84.937342 0.0000000\n10.3-21.1  -60.65000000 -112.662658   -8.637342 0.0087505\n21.3-21.1   92.55000000   40.537342  144.562658 0.0000019\n0.4-21.1  -136.86111111 -190.622580  -83.099642 0.0000000\n10.4-21.1  -50.86111111 -104.622580    2.900358 0.0820969\n21.4-21.1   60.80555556    7.044087  114.567025 0.0129598\n10.2-0.2    67.80000000   10.097136  125.502864 0.0079020\n21.2-0.2   174.00000000  116.297136  231.702864 0.0000000\n0.3-0.2      0.10000000  -57.602864   57.802864 1.0000000\n10.3-0.2    76.40000000   18.697136  134.102864 0.0012895\n21.3-0.2   229.60000000  171.897136  287.302864 0.0000000\n0.4-0.2      0.18888889  -59.095168   59.472946 1.0000000\n10.4-0.2    86.18888889   26.904832  145.472946 0.0002290\n21.4-0.2   197.85555556  138.571499  257.139612 0.0000000\n21.2-10.2  106.20000000   48.497136  163.902864 0.0000007\n0.3-10.2   -67.70000000 -125.402864   -9.997136 0.0080606\n10.3-10.2    8.60000000  -49.102864   66.302864 0.9999974\n21.3-10.2  161.80000000  104.097136  219.502864 0.0000000\n0.4-10.2   -67.61111111 -126.895168   -8.327054 0.0116735\n10.4-10.2   18.38888889  -40.895168   77.672946 0.9965974\n21.4-10.2  130.05555556   70.771499  189.339612 0.0000000\n0.3-21.2  -173.90000000 -231.602864 -116.197136 0.0000000\n10.3-21.2  -97.60000000 -155.302864  -39.897136 0.0000072\n21.3-21.2   55.60000000   -2.102864  113.302864 0.0701848\n0.4-21.2  -173.81111111 -233.095168 -114.527054 0.0000000\n10.4-21.2  -87.81111111 -147.095168  -28.527054 0.0001566\n21.4-21.2   23.85555556  -35.428501   83.139612 0.9718800\n10.3-0.3    76.30000000   18.597136  134.002864 0.0013184\n21.3-0.3   229.50000000  171.797136  287.202864 0.0000000\n0.4-0.3      0.08888889  -59.195168   59.372946 1.0000000\n10.4-0.3    86.08888889   26.804832  145.372946 0.0002344\n21.4-0.3   197.75555556  138.471499  257.039612 0.0000000\n21.3-10.3  153.20000000   95.497136  210.902864 0.0000000\n0.4-10.3   -76.21111111 -135.495168  -16.927054 0.0020987\n10.4-10.3    9.78888889  -49.495168   69.072946 0.9999923\n21.4-10.3  121.45555556   62.171499  180.739612 0.0000000\n0.4-21.3  -229.41111111 -288.695168 -170.127054 0.0000000\n10.4-21.3 -143.41111111 -202.695168  -84.127054 0.0000000\n21.4-21.3  -31.74444444  -91.028501   27.539612 0.8233947\n10.4-0.4    86.00000000   25.175841  146.824159 0.0003949\n21.4-0.4   197.66666667  136.842508  258.490826 0.0000000\n21.4-10.4  111.66666667   50.842508  172.490826 0.0000008\n\n\n組み合わせが多くて解釈するのが大変ですね。 こういう時に事前に作成したグラフなどで全体像を確認しておけば、どの値を特に見るべきかとかが分かります。 今回では、餌ごとの平均体重が時系列でクロスしている部分があれば、「最初はこの餌の方が成長が早いけど、途中からはこの餌の方がいいかも」みたいな考察に持っていける感じがします。\nというか、時間的な成長を比較してもあまり意味がない気がします。\nあまりデータの選択あが正しくなかったかもしれません。 申し訳ありません。\n一旦やり方自体は共有できたかなと思います。 もっとまともなデータを探してきます。 では、今日はこの辺で。"
  },
  {
    "objectID": "posts/2025-5-1-aov3.html#サンプルデータ",
    "href": "posts/2025-5-1-aov3.html#サンプルデータ",
    "title": "分散分析 第3回：繰り返しのある分散分析(反復測定分散分析)",
    "section": "",
    "text": "今回使用するデータはChickWeightです。 このブログで取り上げるのは初めてになると思います。\n\ndata(ChickWeight)\nsummary(ChickWeight)\n\n     weight           Time           Chick     Diet   \n Min.   : 35.0   Min.   : 0.00   13     : 12   1:220  \n 1st Qu.: 63.0   1st Qu.: 4.00   9      : 12   2:120  \n Median :103.0   Median :10.00   20     : 12   3:120  \n Mean   :121.8   Mean   :10.72   10     : 12   4:118  \n 3rd Qu.:163.8   3rd Qu.:16.00   17     : 12          \n Max.   :373.0   Max.   :21.00   19     : 12          \n                                 (Other):506          \n\n\n\nweight : 測定されたヒヨコの体重(数値)\nTime : 体重が測定された日数(数値)\nChick : ヒヨコの個体番号(名義)\nDiet : ヒヨコに与えられた餌の種類(因子)\n\nt検定での”繰り返しのある”という表現がありましたが、同一の調査対象に対する分析において使われることでした。 今回の例でも、ヒヨコの成長の記録のデータの分析に”繰り返しのある”分散分析を行うわけです。\n最もよく使う場面が、事前(pre)・事後(pos)の比較ではないでしょうか。\n体重が測定された日数がどうなっているかも見てみましょう。\n\nunique(ChickWeight$Time)\n\n [1]  0  2  4  6  8 10 12 14 16 18 20 21\n\n\n1日おきに計測している感じですね。 最後だけは連続ですが。"
  },
  {
    "objectID": "posts/2025-5-1-aov3.html#分析計画",
    "href": "posts/2025-5-1-aov3.html#分析計画",
    "title": "分散分析 第3回：繰り返しのある分散分析(反復測定分散分析)",
    "section": "",
    "text": "ChickWeight のデータを分散分析をしていきます。\n独立変数は、Time と Diet で 従属変数が、weightです。\nつまり、繰り返しのある二元配置分散分析ということになります。 与える餌の種類によるヒヨコの体重増加の違いを仮定した分析になります。\n一気に分散分析をしても良いですが、データの可視化をして結果を想像しながら進められたら良いなと思います。\n\nデータの可視化\n前処理\n繰り返しのある分散分析\n結果の可視化"
  },
  {
    "objectID": "posts/2025-5-1-aov3.html#分析",
    "href": "posts/2025-5-1-aov3.html#分析",
    "title": "分散分析 第3回：繰り返しのある分散分析(反復測定分散分析)",
    "section": "",
    "text": "実際に分析を進めていきましょう。\n\n\nまずは餌の種類ごとに色分けしたヒヨコの体重増加を線グラフに表してみましょう。\n\nlibrary(ggplot2)\n\nggplot(ChickWeight, aes(x = Time, y = weight, group = Chick, color = Diet)) +\n  geom_line(alpha = 0.5) +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"各ヒヨコの体重変化（Diet別）\",\n       x = \"日数（Time）\",\n       y = \"体重（g）\",\n       color = \"餌の種類（Diet）\")\n\n\n\n\n\n\n\n\nなんか見づらいですね。 1匹ずつのグラフなので見辛いのかもしれません。 餌毎の平均線を強調して入れてみましょう。\n\nggplot(ChickWeight, aes(x = Time, y = weight, group = Chick)) +\n  geom_line(alpha = 0.4, color = \"gray\") +\n  stat_summary(aes(group = Diet, color = Diet), fun = mean, geom = \"line\", size = 1.2) +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"各ヒヨコの体重とDietごとの平均曲線\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nこれで大分見やすくなりましたね。 平均値では差がありそうですが、21日目での体重にはかなりの差がありそうです。 また、途中で何かしらの原因(死亡とか？)で21日前に切れているデータもありあそうです。\n\n\n\n前処理として、データの加工を行なっていきます。 まずTimeが現在数値になっており、このままでは使用できないのでfactor(要因)型に変換しましょう。\n\nChickWeight$Time &lt;- as.factor(ChickWeight$Time)\n\nこれで、Timeがfactor(要因)型になりました。 しかし、このままでは12水準あり、多重比較やその後の解釈が大変になりそうです。 そこで今回は、開始ー中間ー最終の３地点のみでの比較にしましょう。\nfactor(要因)型だと処理がうまくいかないようなので、一旦数値型に戻しておきましょう。 この話は後日記事にしようと思います。\n\n# まずTimeを数値に戻しておく（もともとfactorにしていた場合）\nChickWeight$Time &lt;- as.numeric(as.character(ChickWeight$Time))\n\n# 0, 10, 21日目のデータだけ抽出\nchick_subset &lt;- subset(ChickWeight, Time %in% c(0, 10, 21))\n\n# Timeをfactorに変換する\nchick_subset$Time &lt;- as.factor(chick_subset$Time)\n\nいろいろと処理をしたので、現在のデータが意図した形になっているかを確認してみましょう。\nサブセット後の基本構造を確認\n\nstr(chick_subset)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  144 obs. of  4 variables:\n $ weight: num  42 93 205 40 103 215 43 99 202 42 ...\n $ Time  : Factor w/ 3 levels \"0\",\"10\",\"21\": 1 2 3 1 2 3 1 2 3 1 ...\n $ Chick : Ord.factor w/ 50 levels \"18\"&lt;\"16\"&lt;\"15\"&lt;..: 15 15 15 17 17 17 14 14 14 11 ...\n $ Diet  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(chick_subset)\n\n     weight      Time        Chick     Diet  \n Min.   : 39.0   0 :50   13     :  3   1:55  \n 1st Qu.: 42.0   10:49   9      :  3   2:30  \n Median :102.5   21:45   20     :  3   3:30  \n Mean   :119.3           10     :  3   4:29  \n 3rd Qu.:157.2           17     :  3         \n Max.   :373.0           19     :  3         \n                         (Other):126         \n\n\nTime列が factor になっているか\n\nclass(chick_subset$Time)      # \"factor\" になっていればOK\n\n[1] \"factor\"\n\nis.factor(chick_subset$Time)  # TRUE ならOK\n\n[1] TRUE\n\n\nTime の水準を確認\n\nlevels(chick_subset$Time)\n\n[1] \"0\"  \"10\" \"21\"\n\ntable(chick_subset$Time)  # 各水準に何件あるかも同時に確認\n\n\n 0 10 21 \n50 49 45 \n\n\nChick が factor になっているか\n\nis.factor(chick_subset$Chick)\n\n[1] TRUE\n\nlevels(chick_subset$Chick)\n\n [1] \"18\" \"16\" \"15\" \"13\" \"9\"  \"20\" \"10\" \"8\"  \"17\" \"19\" \"4\"  \"6\"  \"11\" \"3\"  \"1\" \n[16] \"12\" \"2\"  \"5\"  \"14\" \"7\"  \"24\" \"30\" \"22\" \"23\" \"27\" \"28\" \"26\" \"25\" \"29\" \"21\"\n[31] \"33\" \"37\" \"36\" \"31\" \"39\" \"38\" \"32\" \"40\" \"34\" \"35\" \"44\" \"45\" \"43\" \"41\" \"47\"\n[46] \"49\" \"46\" \"50\" \"42\" \"48\"\n\n\nさて、一気に確認しましたが、その中で気になることが1つあります。 それは、”Time の水準を確認”をした際に、欠損値が出ていることです。 繰り返しのある分散分析の場合は、データが完全であることが前提なので、欠損値があると分析時にエラーになってしまいます。 選択肢としては、下記があります - 欠損のあるデータを除外する - 欠損補填をする(推定値で補完) - 他の分析にする\nということで、今回は最も一般的な方法である欠損値の除外をしてしまいましょう。\n\n# 各 Chick が Time = 0, 10, 21 をすべて持っているかを確認\nlibrary(dplyr)\n\nWarning: パッケージ 'dplyr' はバージョン 4.2.3 の R の下で造られました\n\n\n\n 次のパッケージを付け加えます: 'dplyr' \n\n\n 以下のオブジェクトは 'package:stats' からマスクされています:\n\n    filter, lag\n\n\n 以下のオブジェクトは 'package:base' からマスクされています:\n\n    intersect, setdiff, setequal, union\n\ncomplete_chicks &lt;- chick_subset %&gt;%\n  group_by(Chick) %&gt;%\n  summarise(n = n()) %&gt;%\n  filter(n == 3) %&gt;%\n  pull(Chick)\n\n# 完全なヒヨコだけを抽出\nchick_complete &lt;- chick_subset %&gt;%\n  filter(Chick %in% complete_chicks) %&gt;%\n  droplevels()\n\nさてデータを確認してみましょう。\n\nlevels(chick_complete$Time)\n\n[1] \"0\"  \"10\" \"21\"\n\ntable(chick_complete$Time)  # 各水準に何件あるかも同時に確認\n\n\n 0 10 21 \n45 45 45 \n\n\nこれで良い感じになりましたね。 では、ここからは正規性と等分散性の確認に入りましょう。\n\n\n\n今回も正規性と等分散性の検定はサクッとです。\n\nlibrary(dplyr)\n\nchick_complete %&gt;%\n  group_by(Time) %&gt;%\n  summarise(p_value = shapiro.test(weight)$p.value)\n\n# A tibble: 3 × 2\n  Time   p_value\n  &lt;fct&gt;    &lt;dbl&gt;\n1 0     0.000752\n2 10    0.465   \n3 21    0.869   \n\n\nTimeが0の場合の正規性に問題がありそうです。 Q-Qプロットで見てみましょう。\n\nlibrary(ggplot2)\n\nggplot(chick_complete %&gt;% filter(Time == 0), aes(sample = weight)) +\n  stat_qq() +\n  stat_qq_line() +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"Time = 0 の Q-Qプロット\",\n       x = \"理論分位点\", y = \"観測分位点\")\n\n\n\n\n\n\n\n\n判断しづらいところですね。 一旦はこのまま進めましょう。\n次は等分散性の確認です。\n\nbartlett.test(weight ~ Time, data = chick_complete)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  weight by Time\nBartlett's K-squared = 330.44, df = 2, p-value &lt; 2.2e-16\n\n\n等分散性に問題ありですね。 これが論文であれば、分散分析をやめて他の分析に変更しないといけなくなりそうです。 線形混合モデルとかになるみたいですが、それはまた次回にしましょう。\n今回は繰り返しのある分散分析のやり方を紹介したいので、この問題には目を瞑りましょう。 すみません。\n\n\n\n問題を抱えつつ分析をしてみましょう。\n繰り返しがある場合は、指定が少し面倒なので一度時間のみの１要因で3水準での分析をしてみましょう。\n\n# Chick ごとに Time に沿って繰り返し測定されている構造を Error(Chick/Time) で指定しています。\naov_result &lt;- aov(weight ~ Time + Error(Chick/Time), data = chick_complete)\nsummary(aov_result)\n\n\nError: Chick\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 44 114242    2596               \n\nError: Chick:Time\n          Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTime       2 721615  360808   238.6 &lt;2e-16 ***\nResiduals 88 133068    1512                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n時間による成長は当然のごとく出ますね。 繰り返し測定をしている要因をErrorを指定しているのがポイントです。\nでは、２要因の場合をやってみましょう。\n\naov2 &lt;- aov(weight ~ Time * Diet + Error(Chick/Time), data = chick_complete)\nsummary(aov2)\n\n\nError: Chick\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet       3  30604   10201   5.001 0.00478 **\nResiduals 41  83637    2040                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Chick:Time\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTime       2 721615  360808 293.492  &lt; 2e-16 ***\nTime:Diet  6  32260    5377   4.374 0.000701 ***\nResiduals 82 100808    1229                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n結果の見方はこれまでの分散分析と変わらないので、詳細は省きます(以前の分散分析の記事を参照)。 結果を要約します。 - 餌の種類の主効果あり - 時間による主効果あり - 餌と時間による交互作用あり\nでした。 ここからは多重比較をしましょう。\n\n\n\n前回やった二要因分散分析では多重比較でTukeyHSD()を使用しましたが、繰り返しのある分散分析はこれが使えません。 繰り返しがあることをオプションで使えないようです。\nそのため、Time × Diet の組み合わせを新しい要因として作成して1要因12水準の多重比較の分析に変えます。\n組み合わせでの新しい要因の作成\n\nchick_complete$group &lt;- interaction(chick_complete$Time, chick_complete$Diet)\ntable(chick_complete$group)\n\n\n 0.1 10.1 21.1  0.2 10.2 21.2  0.3 10.3 21.3  0.4 10.4 21.4 \n  16   16   16   10   10   10   10   10   10    9    9    9 \n\n\nこれで新しい要因は作れたので、多重比較を行いましょう。\n\noneway_model &lt;- aov(weight ~ group, data = chick_complete)\nTukeyHSD(oneway_model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ group, data = chick_complete)\n\n$group\n                   diff         lwr         upr     p adj\n10.1-0.1    55.68750000   10.069381  101.305619 0.0046278\n21.1-0.1   136.18750000   90.569381  181.805619 0.0000000\n0.2-0.1     -0.86250000  -52.875158   51.150158 1.0000000\n10.2-0.1    66.93750000   14.924842  118.950158 0.0020618\n21.2-0.1   173.13750000  121.124842  225.150158 0.0000000\n0.3-0.1     -0.76250000  -52.775158   51.250158 1.0000000\n10.3-0.1    75.53750000   23.524842  127.550158 0.0002339\n21.3-0.1   228.73750000  176.724842  280.750158 0.0000000\n0.4-0.1     -0.67361111  -54.435080   53.087858 1.0000000\n10.4-0.1    85.32638889   31.564920  139.087858 0.0000343\n21.4-0.1   196.99305556  143.231587  250.754525 0.0000000\n21.1-10.1   80.50000000   34.881881  126.118119 0.0000023\n0.2-10.1   -56.55000000 -108.562658   -4.537342 0.0207934\n10.2-10.1   11.25000000  -40.762658   63.262658 0.9998829\n21.2-10.1  117.45000000   65.437342  169.462658 0.0000000\n0.3-10.1   -56.45000000 -108.462658   -4.437342 0.0212194\n10.3-10.1   19.85000000  -32.162658   71.862658 0.9811669\n21.3-10.1  173.05000000  121.037342  225.062658 0.0000000\n0.4-10.1   -56.36111111 -110.122580   -2.599642 0.0310837\n10.4-10.1   29.63888889  -24.122580   83.400358 0.7943625\n21.4-10.1  141.30555556   87.544087  195.067025 0.0000000\n0.2-21.1  -137.05000000 -189.062658  -85.037342 0.0000000\n10.2-21.1  -69.25000000 -121.262658  -17.237342 0.0011729\n21.2-21.1   36.95000000  -15.062658   88.962658 0.4361857\n0.3-21.1  -136.95000000 -188.962658  -84.937342 0.0000000\n10.3-21.1  -60.65000000 -112.662658   -8.637342 0.0087505\n21.3-21.1   92.55000000   40.537342  144.562658 0.0000019\n0.4-21.1  -136.86111111 -190.622580  -83.099642 0.0000000\n10.4-21.1  -50.86111111 -104.622580    2.900358 0.0820969\n21.4-21.1   60.80555556    7.044087  114.567025 0.0129598\n10.2-0.2    67.80000000   10.097136  125.502864 0.0079020\n21.2-0.2   174.00000000  116.297136  231.702864 0.0000000\n0.3-0.2      0.10000000  -57.602864   57.802864 1.0000000\n10.3-0.2    76.40000000   18.697136  134.102864 0.0012895\n21.3-0.2   229.60000000  171.897136  287.302864 0.0000000\n0.4-0.2      0.18888889  -59.095168   59.472946 1.0000000\n10.4-0.2    86.18888889   26.904832  145.472946 0.0002290\n21.4-0.2   197.85555556  138.571499  257.139612 0.0000000\n21.2-10.2  106.20000000   48.497136  163.902864 0.0000007\n0.3-10.2   -67.70000000 -125.402864   -9.997136 0.0080606\n10.3-10.2    8.60000000  -49.102864   66.302864 0.9999974\n21.3-10.2  161.80000000  104.097136  219.502864 0.0000000\n0.4-10.2   -67.61111111 -126.895168   -8.327054 0.0116735\n10.4-10.2   18.38888889  -40.895168   77.672946 0.9965974\n21.4-10.2  130.05555556   70.771499  189.339612 0.0000000\n0.3-21.2  -173.90000000 -231.602864 -116.197136 0.0000000\n10.3-21.2  -97.60000000 -155.302864  -39.897136 0.0000072\n21.3-21.2   55.60000000   -2.102864  113.302864 0.0701848\n0.4-21.2  -173.81111111 -233.095168 -114.527054 0.0000000\n10.4-21.2  -87.81111111 -147.095168  -28.527054 0.0001566\n21.4-21.2   23.85555556  -35.428501   83.139612 0.9718800\n10.3-0.3    76.30000000   18.597136  134.002864 0.0013184\n21.3-0.3   229.50000000  171.797136  287.202864 0.0000000\n0.4-0.3      0.08888889  -59.195168   59.372946 1.0000000\n10.4-0.3    86.08888889   26.804832  145.372946 0.0002344\n21.4-0.3   197.75555556  138.471499  257.039612 0.0000000\n21.3-10.3  153.20000000   95.497136  210.902864 0.0000000\n0.4-10.3   -76.21111111 -135.495168  -16.927054 0.0020987\n10.4-10.3    9.78888889  -49.495168   69.072946 0.9999923\n21.4-10.3  121.45555556   62.171499  180.739612 0.0000000\n0.4-21.3  -229.41111111 -288.695168 -170.127054 0.0000000\n10.4-21.3 -143.41111111 -202.695168  -84.127054 0.0000000\n21.4-21.3  -31.74444444  -91.028501   27.539612 0.8233947\n10.4-0.4    86.00000000   25.175841  146.824159 0.0003949\n21.4-0.4   197.66666667  136.842508  258.490826 0.0000000\n21.4-10.4  111.66666667   50.842508  172.490826 0.0000008\n\n\n組み合わせが多くて解釈するのが大変ですね。 こういう時に事前に作成したグラフなどで全体像を確認しておけば、どの値を特に見るべきかとかが分かります。 今回では、餌ごとの平均体重が時系列でクロスしている部分があれば、「最初はこの餌の方が成長が早いけど、途中からはこの餌の方がいいかも」みたいな考察に持っていける感じがします。\nというか、時間的な成長を比較してもあまり意味がない気がします。\nあまりデータの選択あが正しくなかったかもしれません。 申し訳ありません。\n一旦やり方自体は共有できたかなと思います。 もっとまともなデータを探してきます。 では、今日はこの辺で。"
  },
  {
    "objectID": "posts/2025-4-23-can-the-laboratory-method-leave-the-lab.html",
    "href": "posts/2025-4-23-can-the-laboratory-method-leave-the-lab.html",
    "title": "ラボラトリー方式はラボを出られるか",
    "section": "",
    "text": "私は大学時代にラボラトリー方式の体験学習というものを授業で何度も体験しました。 それ自体は面白い経験でしたし、今でも価値が一定あったと思っています。 しかし、大学を卒業して社会に出てから違和感を感じる部分もありました。 大学自体の友人とラボラトリー方式の体験学習について話す機会があったので、そこで考えたこと・思ったことについてまとめてみました。"
  },
  {
    "objectID": "posts/2025-4-23-can-the-laboratory-method-leave-the-lab.html#ラボラトリー方式の体験学習とは何か",
    "href": "posts/2025-4-23-can-the-laboratory-method-leave-the-lab.html#ラボラトリー方式の体験学習とは何か",
    "title": "ラボラトリー方式はラボを出られるか",
    "section": "ラボラトリー方式の体験学習とは何か",
    "text": "ラボラトリー方式の体験学習とは何か\n専門的な話は書籍などに任せますが、ラボラトリー方式の体験学習とは、 「特別に設計された人と人とが関わる場 において、参加者自身の行動や関係性を素材にしながら、 そこでの体験を通し て人間関係を学ぶ方法」です(出典)。 教科書などの教材があるわけではなく、特別に設計された人と人が関わる場(ラボ)での行動や感情などを題材に人間関係を学ぶわけです。人間関係といっていますが、自分自身について気づく場でもあります。 この説明だけではあまり分からないと思いますので、もう少し具体的に説明します。 基本的な流れは下記のような感じです。 - グループワーク - 振り返り - 振り返りの共有\nまずは、就活でやるようなグループワークを行います。 グループワーク中はその中でワークと並行して、その中で起こっていることに注目します。 グループワーク後に、そのワーク中に何が起きていたのかを個人で振り返ります。 その後、振り返りの内容をグループで共有します。 グループワークの中で、誰かの言った一言が方向を決めたり、自然と役割分担がでたり、そう言ったグループ内で起きた動きや、誰かの言った発言で自分は発言しづらくなったとか、そう言った個人の中での感情の動きなども気付きになります。\nこのワーク自体は結構面白いと思います。"
  },
  {
    "objectID": "posts/2025-4-23-can-the-laboratory-method-leave-the-lab.html#ラボラトリー方式の体験学習のメリット",
    "href": "posts/2025-4-23-can-the-laboratory-method-leave-the-lab.html#ラボラトリー方式の体験学習のメリット",
    "title": "ラボラトリー方式はラボを出られるか",
    "section": "ラボラトリー方式の体験学習のメリット",
    "text": "ラボラトリー方式の体験学習のメリット\nここからは私が今考えるラボラトリー方式の体験学習の良かった点です。 一般的に言われていることと違っても、これは個人の感じたことなので許してください。\n\n自分に敏感になれる\n最も価値のあった成長は自分に感情に敏感になれたことです。意外と自分の気持ちについて気が付かないものです。 自分がイライラしているのか、悲しいのか、怒っているのか、そう言ったネガティブな感情を弁別するのは難しいことが多いです。それをリアルタイム、もしくは早い段階で検知できると他人との軋轢をある程度カバーすることができます。\n\n\n挑戦的なことができる\nラボラトリー方式のいい部分は、日常から分離された環境であるからそこいつも自分がやらない挑戦的な姿勢を試すことができることです。 ワークの中では、ロールで指定されるものもあり、普段自分がやらないロールを実践することで道の自分を検証することができます。日常生活でやると「あいつキャラが変わった」みたいに思われることもあります。しかし、日常から切り離された環境だからこそ、新しい自分を試すことができます。\n\n\n他者評価(フィードバック)を得られる\n「自分の振る舞いが他者からどのように見られているのか」、実社会ではこの情報を得ることが難しいです。特に社会人になると、自分に耳の痛いことを言ってくれる人は少なくなります。気が合う人とは仲良くしますが、気が合わない人とは適度距離を置くようになります。年齢が上がるとさらに自分にフィードバックをくれる人は減ってしまいます。 ラボ内では、このフィードバックがたくさん得られることが大きなメリットでした。グループワーク中、自分が考え込んでしまっている時に、周りは私が不機嫌だったりネガティブな状況だと認識することがあるというのは、私の大きな気づきでした。自分のちょっとした癖が、意外と他者にとっては気になる仕草だったりします。"
  },
  {
    "objectID": "posts/2025-4-23-can-the-laboratory-method-leave-the-lab.html#大学の授業での限界",
    "href": "posts/2025-4-23-can-the-laboratory-method-leave-the-lab.html#大学の授業での限界",
    "title": "ラボラトリー方式はラボを出られるか",
    "section": "大学の授業での限界",
    "text": "大学の授業での限界\n\n譲れないものがない\n私は人間関係の軋轢が生まれるのは、譲れないものがある時が多いのだと思ういます。ワークで何かタスクをこなす場合でも、そのタスクをこなす絶対的な必要性がないのです。仕事などでは売り上げへのコミットや自らの成果・責任などの影響を抱えています。家庭では、お金の使い方や子育てなどの譲れない問題があるでしょう。学生でも部活やクラス対抗の学園祭などがあれば、妥協できない強い気持ちがあるでしょう。 このようにタスク自体の優先度が高い場合、よりリアルな衝突が起きます。 しかし、ラボラトリーの中ではそこまで熱量のある問題を作ることができません。だからこそラボな部分はあると思うのですが。社会人がこのワークをやれば、普段はリスクなどの面から挑戦的な新しい姿勢やタスクに没頭してしまうことで見えない関係性(プロセス)をラボラトリーで見ることができると思います。しかし、学生が授業で集中的にラボラトリーをやっても、日常生活への汎用的な気づきにならないように思います。 学生時代にやったラボでの経験と、社会に出てからの実社会での気づきが別物として認識されてしまうのだと思います。\n\n\n同質集団すぎた\n大学でのラボラトリー方式の体験学習は、その参加者の多様性においてとても大きな問題を抱えていたと思います。年齢、学力、親の所得、育成環境がかなり同質な集団が大学には揃ってしまいます。特に、私のいた大学は9割程度は自宅からの通学生で地域的多様性もありませんでした。実社会に出ると、年齢、その集団での職務時間(職場での先輩後輩)、バックグラウンドの違う人たちと関わっていくことになります。 さらに取引先など利害関係も関わりかなり複雑になっていると思います。そう言ったものによる関係性の複雑さやアプローチの難しさは、大学時代はほとんど感じませんでした。 良くも悪くも”良い子”が集まっていたのだと思います。二日酔いで登場してワーク中に眠ったり、「今日はやる気がない」とサボる人もいませんでした。みんなある程度ワークの意図を理解して協力する前提で、ワークに参加します。前提条件が綺麗すぎるのだと思います。 もし今私が大学に戻るなら、ちょっと辺なことをしたいと思います。 また、年齢や状況が違う人たちとやってみたいとも思います。\n\n\n相手にも多くの前提が共有されている\nまた実社会でグループや他者に対して働きかけるようとすると、ラボとは違い相手の受け止め方にも気をつけなければいけません。ラボ内では、働きかけることができても、現実社会ではちょっとした働きかけに、相手が「攻撃された」と認識されていうこともあります。これは、コミュニケーションの本質でもあると思います。コミュニケーションは基本的に発信者・受信者双方に大きな要因があります。発信者がIメッセージを意識しても、受信者がそういったコミュニケーションに慣れていないとキツく言われていると感じてしまうことがあります。 ラボ内では、送受信者双方がグループワークやその後のフィードバックに慣れています。そのため、働きかけがしやすいです。ちょっと踏み込んだ働きかけをしても相手が適宜受け取ってくれます。しかし、実社会にでてしまうとそう簡単ではありません。これは職場などではなく、パートナーとの関係でもそうだと思います。二人の間に起きる問題を向き合ってじっくり話し合える人もいれば、それが苦手で有耶無耶にしたい人もいると思います。 ラボ内では経験しなかった問題が実社会では起き、それが意外と深刻なのだと思います。「どのような働きかけをするか」という問題の中で「働きかける対象がどのように受け取るか」という問題がラボよりも高度な問題になります。"
  },
  {
    "objectID": "posts/2025-4-23-can-the-laboratory-method-leave-the-lab.html#ラボラトリー方式はラボを出られるか",
    "href": "posts/2025-4-23-can-the-laboratory-method-leave-the-lab.html#ラボラトリー方式はラボを出られるか",
    "title": "ラボラトリー方式はラボを出られるか",
    "section": "ラボラトリー方式はラボを出られるか",
    "text": "ラボラトリー方式はラボを出られるか\n私が感じていた主題「ラボラトリー方式はラボを出られるか」。\nつまり、体験や学びがラボ特有のことになってしまい、実社会に適応できないのではないかということです。 ラボ自体は日常から離されたラボであることが大切です。日常から切り離されているからこそ、挑戦したりゆっくり考えたり日常でできないことができるのだと思います。それはタスクにおいても同じことが言えると思います。タスクに没入しすぎないことが、多くの気づきにつながるのだと思います。\nしかしこの学習の本質は、ラボでの学びを日常にフィードバックするということだと思います。ラボでの学びを日常で実践することに価値があるのだと思います。ラボの中では、プロセスに目を向けられるし働きかけられるけど、日常ではそれが全くできないとなってしまうと、それは意味がなくなってしまうと思います。 ただこの問題は研究と社会実装に関する全ての領域に行ける問題だと思います。研究室内では複雑性を排除するために、環境などの要因を整えて1つの事象の研究するので、実社会にそれを適応しようとして時にその複雑性から簡単にはいきません。ラボラトリー方式でも同じことが起きているのかなと思います。参加者の属性や前提知識が整えられていることで、よりプロセスを見たり、働きかけたり、フィードバックをもらうことができます。そう言った意味で、ラボの価値があるのだと思います。ただ、実践的な学習であるからこそ実社会に活かすことができる方が価値があるのではないかとも思います。\n私自身は大学自体のラボラトリー方式の学びを積極的に日常に活かしていると思います。 パートナーと対話する時間も意図的に作りますし、Iメッセージで伝えるようにもしています。 しかし、ラボでの学びを卒業生のどれほどの人数ができているのか。 大学時代の授業の時にもっとラボと日常を繋ぐための工夫はできないのか。 そんなことを考えています。\nもし私の後輩がこの記事を読んでいたら、ラボでの学びを日常にどう生かせるのかなんて考えてみてほしいです。 学生なら、バイト先やサークルなど学科の授業ではない部分で何が実践できるのかを考え、実践してみてほしいです。 そして、ラボでは積極的に”いつも”じゃない自分を演じてみてほしいです。 そもそも大学自体が巨大なラボです。たくさん挑戦して失敗できる環境はとても貴重です。 ぜひ有効活用してみてください。"
  },
  {
    "objectID": "posts/2025-4-19-basic-bata-visualize.html",
    "href": "posts/2025-4-19-basic-bata-visualize.html",
    "title": "基本的なデータの可視化をしてみる",
    "section": "",
    "text": "せっかくRを使うならデータの可視化もやりたいところですよね。 ということで、今回は基本的なデータの可視化をやってみたいと思います。"
  },
  {
    "objectID": "posts/2025-4-19-basic-bata-visualize.html#箱ひげ図を改造する",
    "href": "posts/2025-4-19-basic-bata-visualize.html#箱ひげ図を改造する",
    "title": "基本的なデータの可視化をしてみる",
    "section": "箱ひげ図を改造する",
    "text": "箱ひげ図を改造する\nt検定をする前にの中で箱ひげ図の作成は行いましたが、もっと見やすくしてみましょう。\n外れ値の含まれるcars のデータを使用して、図に名前やラベルをつけたり、色をつけてみやすくしてみます。\n\nstr(cars)\n\n'data.frame':   50 obs. of  2 variables:\n $ speed: num  4 4 7 7 8 9 10 10 10 11 ...\n $ dist : num  2 10 4 22 16 10 18 26 34 17 ...\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\npar(family = \"Hiragino Sans\") # 日本語フォントの指定\nboxplot(cars$dist,\n        main = \"制動距離の箱ひげ図（外れ値つき）\",  # グラフタイトル\n        ylab = \"距離（フィート）\",                   # y軸のラベル\n        col = \"lightblue\",                           # 箱の背景色\n        border = \"darkblue\",                         # 箱の枠線の色\n        boxwex = 0.5,                                # 箱の横幅を調整（デフォルト=0.8）\n        notch = TRUE,                                # 中央のくびれを表示（中央値の信頼区間を視覚化）\n        outline = TRUE,                              # 外れ値を○で描画（デフォルトでTRUE）\n        pch = 19,                                     # 外れ値のマーカーの形（19 = 塗りつぶし丸）\n        cex = 1.2,                                    # 外れ値マーカーのサイズ（標準=1）\n        outcol = \"red\",                               # 外れ値の色\n        horizontal = FALSE                           # 垂直表示（TRUEにすれば水平表示になる）\n)\n\n\n\n\n\n\n\n\n色自体はレポートにするときはあまり使わないかもしれません。ただビジネスの場などでの資料と考えると見やすくなるかもしれませんね。 また、中央値の信頼区間をくびれで表現したりするのは面白いかなと思ったりします。"
  },
  {
    "objectID": "posts/2025-4-19-basic-bata-visualize.html#散布図",
    "href": "posts/2025-4-19-basic-bata-visualize.html#散布図",
    "title": "基本的なデータの可視化をしてみる",
    "section": "散布図",
    "text": "散布図\n次は散布図です。 散布図は2つの変数の関係を可視化するもので、データのばらつきや相関などを可視化できます。\n早速先ほどと同様に cars のデータで見ていきましょう。\n\npar(family = \"Hiragino Sans\") # 日本語フォントの指定\nplot(dist ~ speed, data = cars,\n     main = \"速度と制動距離の関係\",\n     xlab = \"速度（mph）\", ylab = \"制動距離（ft）\",\n     pch = 19, col = \"blue\")\nabline(lm(dist ~ speed, data = cars), col = \"red\")  # 回帰線\n\n\n\n\n\n\n\n\nなんだか相関がありそうな図ですね。 これでもちょっと外れ値がありそうなのが見て取れます。"
  },
  {
    "objectID": "posts/2025-4-19-basic-bata-visualize.html#ヒストグラム",
    "href": "posts/2025-4-19-basic-bata-visualize.html#ヒストグラム",
    "title": "基本的なデータの可視化をしてみる",
    "section": "ヒストグラム",
    "text": "ヒストグラム\nヒストグラムではデータの偏りを見ることができます。 一旦 cars の制動距離の分布を見てみましょう。\n\npar(family = \"Hiragino Sans\") # 日本語フォントの指定\nhist(cars$dist,\n     main = \"制動距離の分布\",\n     xlab = \"制動距離（フィート）\",\n     col = \"lightblue\", # 棒（ビン）の色\n     border = \"white\" # 棒の枠線の色\n     )\n\n\n\n\n\n\n\n\nなんとなく綺麗な分布かなというふうに見えます。 オプションで下記の設定も可能です。\n\nbreaks: 棒（ビン）の数を増減する: breaks = 20\nfreq: 縦軸を頻度（TRUE）か密度（FALSE）にする: freq = FALSE\nylim: 縦軸の範囲を指定する: ylim = c(0, 20)\n\nfreqはデータに対する比率になるので、合計が１になるような小数点になっています。\n\npar(family = \"Hiragino Sans\") # 日本語フォントの指定\nhist(cars$dist,\n     freq = FALSE,   # ここがポイント！\n     main = \"制動距離の密度ヒストグラム\",\n     xlab = \"距離（フィート）\",\n     col = \"lightgreen\",\n     border = \"white\")"
  },
  {
    "objectID": "posts/2025-4-19-basic-bata-visualize.html#棒グラフ",
    "href": "posts/2025-4-19-basic-bata-visualize.html#棒グラフ",
    "title": "基本的なデータの可視化をしてみる",
    "section": "棒グラフ",
    "text": "棒グラフ\n棒グラフはExcelなどでも馴染み深いと思いますので、説明は省略です。\nせっかくなので、平均値を計算してそれを棒グラフにします。\n\npar(family = \"Hiragino Sans\") # 日本語フォントの指定\n# 花の種類ごとの Sepal.Length（がく片の長さ）の平均を計算\navg_sepal_length &lt;- tapply(iris$Sepal.Length, iris$Species, mean)\n\n# 平均値で棒グラフを作成\nbarplot(avg_sepal_length,\n        main = \"花の種類ごとのがく片長の平均\",\n        xlab = \"花の種類\",\n        ylab = \"がく片の長さ（cm）\",\n        col = c(\"skyblue\", \"lightgreen\", \"lightcoral\"),\n        border = \"white\")"
  },
  {
    "objectID": "posts/2025-4-19-basic-bata-visualize.html#折れ線",
    "href": "posts/2025-4-19-basic-bata-visualize.html#折れ線",
    "title": "基本的なデータの可視化をしてみる",
    "section": "折れ線",
    "text": "折れ線\n折れ線グラフも一般的なので、説明は省略です。 今回は例として、carsのデータを使用します。\n\npar(family = \"Hiragino Sans\") # 日本語フォントの指定\n# carsデータで速度 vs 制動距離を折れ線グラフにする\nplot(cars$speed, cars$dist,\n     type = \"l\",             # l = line（線だけ）\n     main = \"速度と制動距離の関係\",\n     xlab = \"速度 (mph)\",\n     ylab = \"制動距離 (ft)\",\n     col = \"blue\",\n     lwd = 2)                # 線の太さ\n\n\n\n\n\n\n\n\nなんだか適切ではない気もしますが、一旦これでいきましょう。"
  },
  {
    "objectID": "posts/2025-4-19-basic-bata-visualize.html#ペアプロット",
    "href": "posts/2025-4-19-basic-bata-visualize.html#ペアプロット",
    "title": "基本的なデータの可視化をしてみる",
    "section": "ペアプロット",
    "text": "ペアプロット\n最後はペアプロットです。 ペアプロットは複数の数値変数の組み合わせごとに散布図を描き、相関やパターンを視覚的に確認できる図です。 散布図を一気にたくさん作る感じです。\n\npar(family = \"Hiragino Sans\") # 日本語フォントの指定\n# 基本のpairs()を使ってペアプロット\npairs(iris[, 1:4],       # 数値列だけ使う\n      main = \"irisデータセットのペアプロット\",\n      pch = 21,\n      bg = c(\"red\", \"green3\", \"blue\")[unclass(iris$Species)])\n\n\n\n\n\n\n\n\n一気に難しい研究をやってる感じになりましたね。"
  },
  {
    "objectID": "posts/2025-4-19-basic-bata-visualize.html#最後に",
    "href": "posts/2025-4-19-basic-bata-visualize.html#最後に",
    "title": "基本的なデータの可視化をしてみる",
    "section": "最後に",
    "text": "最後に\n図をうまく使うとデータの概要を自ら理解したり、他人に伝えるときもわかりやすいことがあります。全体をパッとみて「こんな分析してみようかな〜」みたいな方針を決めることにも役立ちます。\n今回はRで標準使用できるライブラリを使用してきました。 これでも十分かもしれませんが、R界隈のデファクトスタンダード的な存在として、ggplot2というライブラリがあります。\n使い込んでいくなら、ggplot2もぜひ使ってみてください。 ggplot2については、またいつか書けたらと思います。 参考までにサンプルを置いておきます。 では今日はこの辺で。\n\n# 最初はinstall.packages(\"ggplot2\")を実行してください。何か聞かれるのでYes\nlibrary(ggplot2)\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  labs(title = \"Sepal Size by Species\",\n       x = \"Sepal Length (cm)\",\n       y = \"Sepal Width (cm)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2025-4-25-aov2.html",
    "href": "posts/2025-4-25-aov2.html",
    "title": "分散分析 第2回：二元配置分散分析",
    "section": "",
    "text": "今回は二元配置の分散分析をやっていきます。 「そもそも二元配置ってなによ」ってことから見ていきましょうか。\n早速今回使用するデータを確認します。\n\n\n今回使用するデータはToothGrowthです。 何度かこのブログでは紹介していますが、念の為データを確認していきましょう。\n\ndata(ToothGrowth)\nsummary(ToothGrowth)\n\n      len        supp         dose      \n Min.   : 4.20   OJ:30   Min.   :0.500  \n 1st Qu.:13.07   VC:30   1st Qu.:0.500  \n Median :19.25           Median :1.000  \n Mean   :18.81           Mean   :1.167  \n 3rd Qu.:25.27           3rd Qu.:2.000  \n Max.   :33.90           Max.   :2.000  \n\n\n\nlen : 歯の伸びた長さ(数値)\nsupp : 接種したサプリの種類(ラベル)\ndose : 投与量(数値)\n\n\n\n\nこのデータを分散分析をしていきます。 独立変数は、supp と dose で 従属変数が、lenです。\n二元配置分散分析では、主効果と交互作用があります。\n\n\n主効果（Main effect）とは、ある1つの独立変数が目的変数（従属変数）に与える影響のことを指します。\nこの分析では、2つの主効果を検定します：\n\nsupp（サプリの種類）が 歯の伸び（len）に与える効果\ndose（投与量）が 歯の伸びに与える効果\n\nそれぞれが 他の因子に関係なく、単独で平均値に差をもたらしているかどうかを確認します。\n\n\n\n交互作用（Interaction effect）とは、2つの独立変数が組み合わさったときに発生する相乗的な影響のことです。\nたとえば：\n\nsupp の効果が 投与量（dose）の値によって変わる\nまたは dose の効果が サプリの種類（supp）によって変わる\n\nこのように、一方の因子の効果が、もう一方の因子の水準によって変化するとき、交互作用があるといいます。\n交互作用が有意であれば、「主効果だけを見る」だけでは不十分で、2つの因子の組み合わせで結果が変わってくることを意味します。\n\n\n\n分散分析では、以下の3つを個別に評価します：\n\nsupp の主効果\ndose の主効果\nsupp × dose の交互作用\n\nこの3つを分けて見ることで、「それぞれの因子がどのように効いているか」「組み合わせが重要なのか」が明確になります。\n\n\n\n\n実際に分析を進めていきましょう。\n\n\n二元配置の分散分析ができるように、doseをカテゴリに変更してみましょう。 数値データをカテゴリに変更する　方法はいろいろあると思います。 等間隔で切ったり、四分位を使ったりなどです。 今回のToothGrowth は実はデータが3つしかありません。\n\nunique(ToothGrowth$dose) # ユニークな値のみを取り出す。\n\n[1] 0.5 1.0 2.0\n\n\nなので、数値データをカテゴリデータに変更します。 下記コードでRの中でのdose の認識を数値からカテゴリに変更できます。\n\nToothGrowth$dose &lt;- as.factor(ToothGrowth$dose)  # doseをカテゴリとして扱う\nsummary(ToothGrowth)\n\n      len        supp     dose   \n Min.   : 4.20   OJ:30   0.5:20  \n 1st Qu.:13.07   VC:30   1  :20  \n Median :19.25           2  :20  \n Mean   :18.81                   \n 3rd Qu.:25.27                   \n Max.   :33.90                   \n\n\n数値からカテゴリに変更したので、summaryの表示で平均値などではなく各カテゴリでの個数が表示されるようになっています。\n\n\n\n今回は二元配置分散分析の練習なので、正規性と等分散性の検定はサクッとです。\n\nlibrary(dplyr)\n\nWarning: パッケージ 'dplyr' はバージョン 4.2.3 の R の下で造られました\n\n\n\n 次のパッケージを付け加えます: 'dplyr' \n\n\n 以下のオブジェクトは 'package:stats' からマスクされています:\n\n    filter, lag\n\n\n 以下のオブジェクトは 'package:base' からマスクされています:\n\n    intersect, setdiff, setequal, union\n\nToothGrowth %&gt;%\n  group_by(supp, dose) %&gt;%\n  summarise(p_value = shapiro.test(len)$p.value)\n\n`summarise()` has grouped output by 'supp'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   supp [2]\n  supp  dose  p_value\n  &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt;\n1 OJ    0.5     0.182\n2 OJ    1       0.415\n3 OJ    2       0.815\n4 VC    0.5     0.170\n5 VC    1       0.270\n6 VC    2       0.919\n\n\n\nToothGrowth$group &lt;- interaction(ToothGrowth$supp, ToothGrowth$dose)\n\nbartlett.test(len ~ group, data = ToothGrowth)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  len by group\nBartlett's K-squared = 6.9273, df = 5, p-value = 0.2261\n\n\n正規性も等分散性も問題がなさそうです。\n\n\n\nデータの整形が完了したので、実際の分析に移ってみましょう。\n\n# 分散分析モデルの作成（主効果 + 交互作用）\nmodel &lt;- aov(len ~ supp * dose, data = ToothGrowth)\n\n# 結果の表示\nsummary(model)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsupp         1  205.4   205.4  15.572 0.000231 ***\ndose         2 2426.4  1213.2  92.000  &lt; 2e-16 ***\nsupp:dose    2  108.3    54.2   4.107 0.021860 *  \nResiduals   54  712.1    13.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n無事結果が表示されたので、出力結果を読み解いていきましょう。\nDf Sum Sq Mean Sq F value   Pr(&gt;F)\n\nDf : 自由度（Degrees of Freedom）\nSum Sq : 平方和（Sum of Squares）\nMean Sq : 平均平方（Sum Sq ÷ Df）\nF value : F統計量（群間の変動 ÷ 群内の変動）\nPr(&gt;F) : p値（有意確率）\n\nsupp               1  205.4   205.4  15.572  0.000231 ***\n\nsupp : サプリの種類（VC or OJ）による主効果の検定をしていることを示しています。\nDf = 1 : サプリは2水準（VCとOJ）なので自由度は1です。\nSum Sq = 205.4 : サプリによるばらつきの平方和\nMean Sq = 205.4 : 自由度1なので平方平均も同じ\nF value = 15.572 : 群間変動（サプリの効果）が群内変動に比べて約15.6倍大きい\nPr(&gt;F) = 0.000231 : p &lt; 0.001なので非常に有意（***）\n\ndose               2 2426.4  1213.2  92.000  &lt; 2e-16 ***\n\ndose : 投与量（0.5, 1, 2）による主効果の検定\nDf = 2 : 3水準 → 自由度2\nSum Sq = 2426.4 : 投与量によるばらつきの平方和\nMean Sq = 1213.2 : 2426.4 ÷ 2\nF value = 92.000 : 群間変動が群内変動に比べて92倍も大きい\nPr(&gt;F) &lt; 2e-16 : 極めて小さいp値（***）\n\nsupp:dose          2  108.3    54.2   4.108  0.021860 *\n\nsupp:dose : サプリと投与量の交互作用効果の検定\nDf = 2 : 組み合わせの自由度2\nSum Sq = 108.3 : 交互作用によるばらつきの平方和\nMean Sq = 54.2 : 108.3 ÷ 2\nF value = 4.108 : 交互作用効果の大きさ（群間 vs 群内変動比）\nPr(&gt;F) = 0.021860 : p &lt; 0.05なので有意（*）\n\nResiduals         54  712.1    13.2\n\nResiduals : 誤差（説明できなかったばらつき）\nDf = 54 : 総サンプル数（60）− 各効果の自由度合計（1+2+2）\nSum Sq = 712.1 : 誤差平方和\nMean Sq = 13.2 : 誤差平方平均\n\n\n\n\n\nsuppに有意に効果あり → サプリの種類の主効果\ndoseに有意に効果あり → 投与量の主効果\nsupp&doseに有意に効果あり → 交互作用\n\nサプリの種類（supp）、投与量（dose）ともに歯の伸びに有意な影響を与えることが分かりました。\nまた、サプリと投与量の間にも有意な交互作用が見られたため、単純な主効果だけでなく、「どのサプリをどの投与量で使用するか」という組み合わせによって結果が変わることが示唆されました。\n\n\n\n\n分散分析で有意差がありましたので、多重比較を行いましょう。 多重比較は、一元配置分散分析でも使用した TukeyのHSD検定 です。\n\n# 分散分析モデル（すでに作成済みのためコメントアウト）\n# model &lt;- aov(len ~ supp * dose, data = ToothGrowth)\n\n# TukeyHSDによる多重比較\ntukey_result &lt;- TukeyHSD(model)\n\n# 結果の表示\ntukey_result\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = len ~ supp * dose, data = ToothGrowth)\n\n$supp\n      diff       lwr       upr     p adj\nVC-OJ -3.7 -5.579828 -1.820172 0.0002312\n\n$dose\n        diff       lwr       upr   p adj\n1-0.5  9.130  6.362488 11.897512 0.0e+00\n2-0.5 15.495 12.727488 18.262512 0.0e+00\n2-1    6.365  3.597488  9.132512 2.7e-06\n\n$`supp:dose`\n               diff        lwr        upr     p adj\nVC:0.5-OJ:0.5 -5.25 -10.048124 -0.4518762 0.0242521\nOJ:1-OJ:0.5    9.47   4.671876 14.2681238 0.0000046\nVC:1-OJ:0.5    3.54  -1.258124  8.3381238 0.2640208\nOJ:2-OJ:0.5   12.83   8.031876 17.6281238 0.0000000\nVC:2-OJ:0.5   12.91   8.111876 17.7081238 0.0000000\nOJ:1-VC:0.5   14.72   9.921876 19.5181238 0.0000000\nVC:1-VC:0.5    8.79   3.991876 13.5881238 0.0000210\nOJ:2-VC:0.5   18.08  13.281876 22.8781238 0.0000000\nVC:2-VC:0.5   18.16  13.361876 22.9581238 0.0000000\nVC:1-OJ:1     -5.93 -10.728124 -1.1318762 0.0073930\nOJ:2-OJ:1      3.36  -1.438124  8.1581238 0.3187361\nVC:2-OJ:1      3.44  -1.358124  8.2381238 0.2936430\nOJ:2-VC:1      9.29   4.491876 14.0881238 0.0000069\nVC:2-VC:1      9.37   4.571876 14.1681238 0.0000058\nVC:2-OJ:2      0.08  -4.718124  4.8781238 1.0000000\n\n\nいつも通り結果を細かくみていきましょう。\nTukey multiple comparisons of means\n95% family-wise confidence level\n\nTukeyのHSD（Honestly Significant Difference）検定で、多群間のペアを比較を実施\n全体の誤差率（family-wise error rate）を5%以内にコントロールした多重比較を実施\n\nFit: aov(formula = len ~ supp * dose, data = ToothGrowth)\n\n分析に使用したモデルを記載してくれています。\n\n$supp\n      diff       lwr       upr     p adj\nVC-OJ -3.7 -5.579828 -1.820172 0.0002312\nここでは、supp(サプリ)での主効果を検定しています。 $suppがsuppの主効果を確認していることを示しています。\nそして、それ以下の項目は下記の通りです。\n\ndiff : 「最初の群 − 次の群」の平均差。正なら最初が大きい、負なら小さい。\nlwr : その差の推定値の信頼できる範囲(95%信頼区間)の下側\nupr : その差の推定値の信頼できる範囲(95%信頼区間)の上側\np adj : 多重比較補正後のp値（通常のp値より厳しくなっている）\n\nこのことから、今回の結果は「OJの方がVCより歯が伸びる効果が有意に高い」と言えます。\n$dose\n        diff       lwr       upr   p adj\n1-0.5  9.130  6.362488 11.897512 0.0e+00\n2-0.5 15.495 12.727488 18.262512 0.0e+00\n2-1    6.365  3.597488  9.132512 2.7e-06\n次はdose(摂取量)の主効果を調べています。 doseは３要因なので、１ついずつ比較して３パターンあり、それぞれを比較しています。 全ての比較で有意差があり、0.5 &lt; 1.0 &lt; 2 という関係になっています。\n$`supp:dose`\n               diff        lwr        upr     p adj\nVC:0.5-OJ:0.5 -5.25 -10.048124 -0.4518762 0.0242521\nOJ:1-OJ:0.5    9.47   4.671876 14.2681238 0.0000046\nVC:1-OJ:0.5    3.54  -1.258124  8.3381238 0.2640208\nOJ:2-OJ:0.5   12.83   8.031876 17.6281238 0.0000000\nVC:2-OJ:0.5   12.91   8.111876 17.7081238 0.0000000\nOJ:1-VC:0.5   14.72   9.921876 19.5181238 0.0000000\nVC:1-VC:0.5    8.79   3.991876 13.5881238 0.0000210\nOJ:2-VC:0.5   18.08  13.281876 22.8781238 0.0000000\nVC:2-VC:0.5   18.16  13.361876 22.9581238 0.0000000\nVC:1-OJ:1     -5.93 -10.728124 -1.1318762 0.0073930\nOJ:2-OJ:1      3.36  -1.438124  8.1581238 0.3187361\nVC:2-OJ:1      3.44  -1.358124  8.2381238 0.2936430\nOJ:2-VC:1      9.29   4.491876 14.0881238 0.0000069\nVC:2-VC:1      9.37   4.571876 14.1681238 0.0000058\nVC:2-OJ:2      0.08  -4.718124  4.8781238 1.0000000\n最後にsupp:dose の交互作用の検定結果です。suppが２要因で、doseが３要因なので2×3で6パターンの組み合わせがあります。その中から2つを選び比較していくので、「順番を考慮しない2つの選び方」の計算になります。 6C2=15通りになります。\n結果は下記の通りです。\n\n\n\n比較\n差 (diff)\np値 (p adj)\n有意差の有無\n\n\n\n\nVC:0.5 - OJ:0.5\n-5.25\n0.0242521\n有意\n\n\nOJ:1 - OJ:0.5\n9.47\n0.0000046\n有意\n\n\nVC:1 - OJ:0.5\n3.54\n0.2640208\n有意差なし\n\n\nOJ:2 - OJ:0.5\n12.83\n0.0000000\n有意\n\n\nVC:2 - OJ:0.5\n12.91\n0.0000000\n有意\n\n\nOJ:1 - VC:0.5\n14.72\n0.0000000\n有意\n\n\nVC:1 - VC:0.5\n8.79\n0.0000210\n有意\n\n\nOJ:2 - VC:0.5\n18.08\n0.0000000\n有意\n\n\nVC:2 - VC:0.5\n18.16\n0.0000000\n有意\n\n\nVC:1 - OJ:1\n-5.93\n0.0073930\n有意\n\n\nOJ:2 - OJ:1\n3.36\n0.3187361\n有意差なし\n\n\nVC:2 - OJ:1\n3.44\n0.2936430\n有意差なし\n\n\nOJ:2 - VC:1\n9.29\n0.0000069\n有意\n\n\nVC:2 - VC:1\n9.37\n0.0000058\n有意\n\n\nVC:2 - OJ:2\n0.08\n1.0000000\n有意差なし\n\n\n\n結果が多くて大変ですが、頑張って細かくみるしかないですね。\n私なりに結果をまとめてみると下記の通りです。\n\nサプリの種類（supp）だけでも有意差あり（OJ &gt; VC）\n投与量（dose）は増えるほど効果あり（すべてのペアで有意）\n交互作用（supp:dose）では、特定の組み合わせで効果が異なる\n特に低投与量（0.5）ではOJが優勢\n高投与量（2.0）ではVCとOJの差がほぼない\n\n\n\n\n\n\nまずは主効果を見るための図を作成してみましょう。 平均＋標準誤差バーの棒グラフを作成してみます。\n\nlibrary(ggplot2)\n\n# supp（サプリ）の主効果\nggplot(ToothGrowth, aes(x = supp, y = len, fill = supp)) +\n  stat_summary(fun = mean, geom = \"bar\", width = 0.6) +    # 平均値の棒グラフ\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +  # 標準誤差バー\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"サプリの種類別：歯の伸びの平均\",\n       x = \"サプリの種類\",\n       y = \"歯の伸び (len)\")\n\n\n\n\n\n\n\n# dose（投与量）の主効果\nggplot(ToothGrowth, aes(x = dose, y = len, fill = dose)) +\n  stat_summary(fun = mean, geom = \"bar\", width = 0.6) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"投与量別：歯の伸びの平均\",\n       x = \"投与量\",\n       y = \"歯の伸び (len)\")\n\n\n\n\n\n\n\n\n今回は棒グラフ以外も作成してみます。 平均を点で示し、標準誤差をバーで表示します。\n\n# dose別に点＋線で描く\nggplot(ToothGrowth, aes(x = dose, y = len, group = 1)) +\n  stat_summary(fun = mean, geom = \"point\", size = 4) +\n  stat_summary(fun = mean, geom = \"line\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"投与量ごとの歯の伸び平均（ポイント＋エラーバー）\",\n       x = \"投与量\",\n       y = \"歯の伸び (len)\")\n\n\n\n\n\n\n\n\n\n\n\n交互作用を可視化する図としては、交互作用プロットを使用することが多いようです。\n\nggplot(ToothGrowth, aes(x = dose, y = len, color = supp, group = supp)) +\n  stat_summary(fun = mean, geom = \"line\", size = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.1) +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"サプリと投与量の交互作用\",\n       x = \"投与量\",\n       y = \"歯の伸び\",\n       color = \"サプリ\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n見方としては、\n\n2本の線（VCとOJ）が描かれます\n線が交差または離れているほど、交互作用がある\n線がほぼ平行なら、交互作用は弱い（またはなし）\n\nという感じです。\n\n本日は二元配置分散分析でした。\n次は繰り返しのあるものですね。ではまた次回。"
  },
  {
    "objectID": "posts/2025-4-25-aov2.html#サンプルデータ",
    "href": "posts/2025-4-25-aov2.html#サンプルデータ",
    "title": "分散分析 第2回：二元配置分散分析",
    "section": "",
    "text": "今回使用するデータはToothGrowthです。 何度かこのブログでは紹介していますが、念の為データを確認していきましょう。\n\ndata(ToothGrowth)\nsummary(ToothGrowth)\n\n      len        supp         dose      \n Min.   : 4.20   OJ:30   Min.   :0.500  \n 1st Qu.:13.07   VC:30   1st Qu.:0.500  \n Median :19.25           Median :1.000  \n Mean   :18.81           Mean   :1.167  \n 3rd Qu.:25.27           3rd Qu.:2.000  \n Max.   :33.90           Max.   :2.000  \n\n\n\nlen : 歯の伸びた長さ(数値)\nsupp : 接種したサプリの種類(ラベル)\ndose : 投与量(数値)"
  },
  {
    "objectID": "posts/2025-4-25-aov2.html#分析計画",
    "href": "posts/2025-4-25-aov2.html#分析計画",
    "title": "分散分析 第2回：二元配置分散分析",
    "section": "",
    "text": "このデータを分散分析をしていきます。 独立変数は、supp と dose で 従属変数が、lenです。\n二元配置分散分析では、主効果と交互作用があります。\n\n\n主効果（Main effect）とは、ある1つの独立変数が目的変数（従属変数）に与える影響のことを指します。\nこの分析では、2つの主効果を検定します：\n\nsupp（サプリの種類）が 歯の伸び（len）に与える効果\ndose（投与量）が 歯の伸びに与える効果\n\nそれぞれが 他の因子に関係なく、単独で平均値に差をもたらしているかどうかを確認します。\n\n\n\n交互作用（Interaction effect）とは、2つの独立変数が組み合わさったときに発生する相乗的な影響のことです。\nたとえば：\n\nsupp の効果が 投与量（dose）の値によって変わる\nまたは dose の効果が サプリの種類（supp）によって変わる\n\nこのように、一方の因子の効果が、もう一方の因子の水準によって変化するとき、交互作用があるといいます。\n交互作用が有意であれば、「主効果だけを見る」だけでは不十分で、2つの因子の組み合わせで結果が変わってくることを意味します。\n\n\n\n分散分析では、以下の3つを個別に評価します：\n\nsupp の主効果\ndose の主効果\nsupp × dose の交互作用\n\nこの3つを分けて見ることで、「それぞれの因子がどのように効いているか」「組み合わせが重要なのか」が明確になります。"
  },
  {
    "objectID": "posts/2025-4-25-aov2.html#分析",
    "href": "posts/2025-4-25-aov2.html#分析",
    "title": "分散分析 第2回：二元配置分散分析",
    "section": "",
    "text": "実際に分析を進めていきましょう。\n\n\n二元配置の分散分析ができるように、doseをカテゴリに変更してみましょう。 数値データをカテゴリに変更する　方法はいろいろあると思います。 等間隔で切ったり、四分位を使ったりなどです。 今回のToothGrowth は実はデータが3つしかありません。\n\nunique(ToothGrowth$dose) # ユニークな値のみを取り出す。\n\n[1] 0.5 1.0 2.0\n\n\nなので、数値データをカテゴリデータに変更します。 下記コードでRの中でのdose の認識を数値からカテゴリに変更できます。\n\nToothGrowth$dose &lt;- as.factor(ToothGrowth$dose)  # doseをカテゴリとして扱う\nsummary(ToothGrowth)\n\n      len        supp     dose   \n Min.   : 4.20   OJ:30   0.5:20  \n 1st Qu.:13.07   VC:30   1  :20  \n Median :19.25           2  :20  \n Mean   :18.81                   \n 3rd Qu.:25.27                   \n Max.   :33.90                   \n\n\n数値からカテゴリに変更したので、summaryの表示で平均値などではなく各カテゴリでの個数が表示されるようになっています。\n\n\n\n今回は二元配置分散分析の練習なので、正規性と等分散性の検定はサクッとです。\n\nlibrary(dplyr)\n\nWarning: パッケージ 'dplyr' はバージョン 4.2.3 の R の下で造られました\n\n\n\n 次のパッケージを付け加えます: 'dplyr' \n\n\n 以下のオブジェクトは 'package:stats' からマスクされています:\n\n    filter, lag\n\n\n 以下のオブジェクトは 'package:base' からマスクされています:\n\n    intersect, setdiff, setequal, union\n\nToothGrowth %&gt;%\n  group_by(supp, dose) %&gt;%\n  summarise(p_value = shapiro.test(len)$p.value)\n\n`summarise()` has grouped output by 'supp'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   supp [2]\n  supp  dose  p_value\n  &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt;\n1 OJ    0.5     0.182\n2 OJ    1       0.415\n3 OJ    2       0.815\n4 VC    0.5     0.170\n5 VC    1       0.270\n6 VC    2       0.919\n\n\n\nToothGrowth$group &lt;- interaction(ToothGrowth$supp, ToothGrowth$dose)\n\nbartlett.test(len ~ group, data = ToothGrowth)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  len by group\nBartlett's K-squared = 6.9273, df = 5, p-value = 0.2261\n\n\n正規性も等分散性も問題がなさそうです。\n\n\n\nデータの整形が完了したので、実際の分析に移ってみましょう。\n\n# 分散分析モデルの作成（主効果 + 交互作用）\nmodel &lt;- aov(len ~ supp * dose, data = ToothGrowth)\n\n# 結果の表示\nsummary(model)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsupp         1  205.4   205.4  15.572 0.000231 ***\ndose         2 2426.4  1213.2  92.000  &lt; 2e-16 ***\nsupp:dose    2  108.3    54.2   4.107 0.021860 *  \nResiduals   54  712.1    13.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n無事結果が表示されたので、出力結果を読み解いていきましょう。\nDf Sum Sq Mean Sq F value   Pr(&gt;F)\n\nDf : 自由度（Degrees of Freedom）\nSum Sq : 平方和（Sum of Squares）\nMean Sq : 平均平方（Sum Sq ÷ Df）\nF value : F統計量（群間の変動 ÷ 群内の変動）\nPr(&gt;F) : p値（有意確率）\n\nsupp               1  205.4   205.4  15.572  0.000231 ***\n\nsupp : サプリの種類（VC or OJ）による主効果の検定をしていることを示しています。\nDf = 1 : サプリは2水準（VCとOJ）なので自由度は1です。\nSum Sq = 205.4 : サプリによるばらつきの平方和\nMean Sq = 205.4 : 自由度1なので平方平均も同じ\nF value = 15.572 : 群間変動（サプリの効果）が群内変動に比べて約15.6倍大きい\nPr(&gt;F) = 0.000231 : p &lt; 0.001なので非常に有意（***）\n\ndose               2 2426.4  1213.2  92.000  &lt; 2e-16 ***\n\ndose : 投与量（0.5, 1, 2）による主効果の検定\nDf = 2 : 3水準 → 自由度2\nSum Sq = 2426.4 : 投与量によるばらつきの平方和\nMean Sq = 1213.2 : 2426.4 ÷ 2\nF value = 92.000 : 群間変動が群内変動に比べて92倍も大きい\nPr(&gt;F) &lt; 2e-16 : 極めて小さいp値（***）\n\nsupp:dose          2  108.3    54.2   4.108  0.021860 *\n\nsupp:dose : サプリと投与量の交互作用効果の検定\nDf = 2 : 組み合わせの自由度2\nSum Sq = 108.3 : 交互作用によるばらつきの平方和\nMean Sq = 54.2 : 108.3 ÷ 2\nF value = 4.108 : 交互作用効果の大きさ（群間 vs 群内変動比）\nPr(&gt;F) = 0.021860 : p &lt; 0.05なので有意（*）\n\nResiduals         54  712.1    13.2\n\nResiduals : 誤差（説明できなかったばらつき）\nDf = 54 : 総サンプル数（60）− 各効果の自由度合計（1+2+2）\nSum Sq = 712.1 : 誤差平方和\nMean Sq = 13.2 : 誤差平方平均\n\n\n\n\n\nsuppに有意に効果あり → サプリの種類の主効果\ndoseに有意に効果あり → 投与量の主効果\nsupp&doseに有意に効果あり → 交互作用\n\nサプリの種類（supp）、投与量（dose）ともに歯の伸びに有意な影響を与えることが分かりました。\nまた、サプリと投与量の間にも有意な交互作用が見られたため、単純な主効果だけでなく、「どのサプリをどの投与量で使用するか」という組み合わせによって結果が変わることが示唆されました。"
  },
  {
    "objectID": "posts/2025-4-25-aov2.html#多重比較",
    "href": "posts/2025-4-25-aov2.html#多重比較",
    "title": "分散分析 第2回：二元配置分散分析",
    "section": "",
    "text": "分散分析で有意差がありましたので、多重比較を行いましょう。 多重比較は、一元配置分散分析でも使用した TukeyのHSD検定 です。\n\n# 分散分析モデル（すでに作成済みのためコメントアウト）\n# model &lt;- aov(len ~ supp * dose, data = ToothGrowth)\n\n# TukeyHSDによる多重比較\ntukey_result &lt;- TukeyHSD(model)\n\n# 結果の表示\ntukey_result\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = len ~ supp * dose, data = ToothGrowth)\n\n$supp\n      diff       lwr       upr     p adj\nVC-OJ -3.7 -5.579828 -1.820172 0.0002312\n\n$dose\n        diff       lwr       upr   p adj\n1-0.5  9.130  6.362488 11.897512 0.0e+00\n2-0.5 15.495 12.727488 18.262512 0.0e+00\n2-1    6.365  3.597488  9.132512 2.7e-06\n\n$`supp:dose`\n               diff        lwr        upr     p adj\nVC:0.5-OJ:0.5 -5.25 -10.048124 -0.4518762 0.0242521\nOJ:1-OJ:0.5    9.47   4.671876 14.2681238 0.0000046\nVC:1-OJ:0.5    3.54  -1.258124  8.3381238 0.2640208\nOJ:2-OJ:0.5   12.83   8.031876 17.6281238 0.0000000\nVC:2-OJ:0.5   12.91   8.111876 17.7081238 0.0000000\nOJ:1-VC:0.5   14.72   9.921876 19.5181238 0.0000000\nVC:1-VC:0.5    8.79   3.991876 13.5881238 0.0000210\nOJ:2-VC:0.5   18.08  13.281876 22.8781238 0.0000000\nVC:2-VC:0.5   18.16  13.361876 22.9581238 0.0000000\nVC:1-OJ:1     -5.93 -10.728124 -1.1318762 0.0073930\nOJ:2-OJ:1      3.36  -1.438124  8.1581238 0.3187361\nVC:2-OJ:1      3.44  -1.358124  8.2381238 0.2936430\nOJ:2-VC:1      9.29   4.491876 14.0881238 0.0000069\nVC:2-VC:1      9.37   4.571876 14.1681238 0.0000058\nVC:2-OJ:2      0.08  -4.718124  4.8781238 1.0000000\n\n\nいつも通り結果を細かくみていきましょう。\nTukey multiple comparisons of means\n95% family-wise confidence level\n\nTukeyのHSD（Honestly Significant Difference）検定で、多群間のペアを比較を実施\n全体の誤差率（family-wise error rate）を5%以内にコントロールした多重比較を実施\n\nFit: aov(formula = len ~ supp * dose, data = ToothGrowth)\n\n分析に使用したモデルを記載してくれています。\n\n$supp\n      diff       lwr       upr     p adj\nVC-OJ -3.7 -5.579828 -1.820172 0.0002312\nここでは、supp(サプリ)での主効果を検定しています。 $suppがsuppの主効果を確認していることを示しています。\nそして、それ以下の項目は下記の通りです。\n\ndiff : 「最初の群 − 次の群」の平均差。正なら最初が大きい、負なら小さい。\nlwr : その差の推定値の信頼できる範囲(95%信頼区間)の下側\nupr : その差の推定値の信頼できる範囲(95%信頼区間)の上側\np adj : 多重比較補正後のp値（通常のp値より厳しくなっている）\n\nこのことから、今回の結果は「OJの方がVCより歯が伸びる効果が有意に高い」と言えます。\n$dose\n        diff       lwr       upr   p adj\n1-0.5  9.130  6.362488 11.897512 0.0e+00\n2-0.5 15.495 12.727488 18.262512 0.0e+00\n2-1    6.365  3.597488  9.132512 2.7e-06\n次はdose(摂取量)の主効果を調べています。 doseは３要因なので、１ついずつ比較して３パターンあり、それぞれを比較しています。 全ての比較で有意差があり、0.5 &lt; 1.0 &lt; 2 という関係になっています。\n$`supp:dose`\n               diff        lwr        upr     p adj\nVC:0.5-OJ:0.5 -5.25 -10.048124 -0.4518762 0.0242521\nOJ:1-OJ:0.5    9.47   4.671876 14.2681238 0.0000046\nVC:1-OJ:0.5    3.54  -1.258124  8.3381238 0.2640208\nOJ:2-OJ:0.5   12.83   8.031876 17.6281238 0.0000000\nVC:2-OJ:0.5   12.91   8.111876 17.7081238 0.0000000\nOJ:1-VC:0.5   14.72   9.921876 19.5181238 0.0000000\nVC:1-VC:0.5    8.79   3.991876 13.5881238 0.0000210\nOJ:2-VC:0.5   18.08  13.281876 22.8781238 0.0000000\nVC:2-VC:0.5   18.16  13.361876 22.9581238 0.0000000\nVC:1-OJ:1     -5.93 -10.728124 -1.1318762 0.0073930\nOJ:2-OJ:1      3.36  -1.438124  8.1581238 0.3187361\nVC:2-OJ:1      3.44  -1.358124  8.2381238 0.2936430\nOJ:2-VC:1      9.29   4.491876 14.0881238 0.0000069\nVC:2-VC:1      9.37   4.571876 14.1681238 0.0000058\nVC:2-OJ:2      0.08  -4.718124  4.8781238 1.0000000\n最後にsupp:dose の交互作用の検定結果です。suppが２要因で、doseが３要因なので2×3で6パターンの組み合わせがあります。その中から2つを選び比較していくので、「順番を考慮しない2つの選び方」の計算になります。 6C2=15通りになります。\n結果は下記の通りです。\n\n\n\n比較\n差 (diff)\np値 (p adj)\n有意差の有無\n\n\n\n\nVC:0.5 - OJ:0.5\n-5.25\n0.0242521\n有意\n\n\nOJ:1 - OJ:0.5\n9.47\n0.0000046\n有意\n\n\nVC:1 - OJ:0.5\n3.54\n0.2640208\n有意差なし\n\n\nOJ:2 - OJ:0.5\n12.83\n0.0000000\n有意\n\n\nVC:2 - OJ:0.5\n12.91\n0.0000000\n有意\n\n\nOJ:1 - VC:0.5\n14.72\n0.0000000\n有意\n\n\nVC:1 - VC:0.5\n8.79\n0.0000210\n有意\n\n\nOJ:2 - VC:0.5\n18.08\n0.0000000\n有意\n\n\nVC:2 - VC:0.5\n18.16\n0.0000000\n有意\n\n\nVC:1 - OJ:1\n-5.93\n0.0073930\n有意\n\n\nOJ:2 - OJ:1\n3.36\n0.3187361\n有意差なし\n\n\nVC:2 - OJ:1\n3.44\n0.2936430\n有意差なし\n\n\nOJ:2 - VC:1\n9.29\n0.0000069\n有意\n\n\nVC:2 - VC:1\n9.37\n0.0000058\n有意\n\n\nVC:2 - OJ:2\n0.08\n1.0000000\n有意差なし\n\n\n\n結果が多くて大変ですが、頑張って細かくみるしかないですね。\n私なりに結果をまとめてみると下記の通りです。\n\nサプリの種類（supp）だけでも有意差あり（OJ &gt; VC）\n投与量（dose）は増えるほど効果あり（すべてのペアで有意）\n交互作用（supp:dose）では、特定の組み合わせで効果が異なる\n特に低投与量（0.5）ではOJが優勢\n高投与量（2.0）ではVCとOJの差がほぼない"
  },
  {
    "objectID": "posts/2025-4-25-aov2.html#図の作成",
    "href": "posts/2025-4-25-aov2.html#図の作成",
    "title": "分散分析 第2回：二元配置分散分析",
    "section": "",
    "text": "まずは主効果を見るための図を作成してみましょう。 平均＋標準誤差バーの棒グラフを作成してみます。\n\nlibrary(ggplot2)\n\n# supp（サプリ）の主効果\nggplot(ToothGrowth, aes(x = supp, y = len, fill = supp)) +\n  stat_summary(fun = mean, geom = \"bar\", width = 0.6) +    # 平均値の棒グラフ\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +  # 標準誤差バー\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"サプリの種類別：歯の伸びの平均\",\n       x = \"サプリの種類\",\n       y = \"歯の伸び (len)\")\n\n\n\n\n\n\n\n# dose（投与量）の主効果\nggplot(ToothGrowth, aes(x = dose, y = len, fill = dose)) +\n  stat_summary(fun = mean, geom = \"bar\", width = 0.6) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"投与量別：歯の伸びの平均\",\n       x = \"投与量\",\n       y = \"歯の伸び (len)\")\n\n\n\n\n\n\n\n\n今回は棒グラフ以外も作成してみます。 平均を点で示し、標準誤差をバーで表示します。\n\n# dose別に点＋線で描く\nggplot(ToothGrowth, aes(x = dose, y = len, group = 1)) +\n  stat_summary(fun = mean, geom = \"point\", size = 4) +\n  stat_summary(fun = mean, geom = \"line\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"投与量ごとの歯の伸び平均（ポイント＋エラーバー）\",\n       x = \"投与量\",\n       y = \"歯の伸び (len)\")\n\n\n\n\n\n\n\n\n\n\n\n交互作用を可視化する図としては、交互作用プロットを使用することが多いようです。\n\nggplot(ToothGrowth, aes(x = dose, y = len, color = supp, group = supp)) +\n  stat_summary(fun = mean, geom = \"line\", size = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.1) +\n  theme_minimal(base_family = \"Hiragino Sans\") +\n  labs(title = \"サプリと投与量の交互作用\",\n       x = \"投与量\",\n       y = \"歯の伸び\",\n       color = \"サプリ\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n見方としては、\n\n2本の線（VCとOJ）が描かれます\n線が交差または離れているほど、交互作用がある\n線がほぼ平行なら、交互作用は弱い（またはなし）\n\nという感じです。\n\n本日は二元配置分散分析でした。\n次は繰り返しのあるものですね。ではまた次回。"
  },
  {
    "objectID": "posts/2025-04-14-r-blog-with-r.html",
    "href": "posts/2025-04-14-r-blog-with-r.html",
    "title": "RでRのブログを作る",
    "section": "",
    "text": "Rの再学習のブログを書くことに決めて、まず最初に考えることはやっぱり「どこでブログを書こうか」ですよね。 はてなブログやQiita, Zennなど多数のサービスがあり簡単にブログは始められます。\nしかしせっかくなので、ブログの作成もRでやってみたい。 私がRをメインで使用していた時には、まだメジャーではありませんでしたがR MarkdownなどのRでレポートを作成したり、簡単なWEBサイトをつくれるという話はありました。 当時はそのようなものを使ったことはありませんでしが、せっかく学習し直すのでその辺もカバーしていこうと思います。"
  },
  {
    "objectID": "posts/2025-04-14-r-blog-with-r.html#背景",
    "href": "posts/2025-04-14-r-blog-with-r.html#背景",
    "title": "RでRのブログを作る",
    "section": "",
    "text": "Rの再学習のブログを書くことに決めて、まず最初に考えることはやっぱり「どこでブログを書こうか」ですよね。 はてなブログやQiita, Zennなど多数のサービスがあり簡単にブログは始められます。\nしかしせっかくなので、ブログの作成もRでやってみたい。 私がRをメインで使用していた時には、まだメジャーではありませんでしたがR MarkdownなどのRでレポートを作成したり、簡単なWEBサイトをつくれるという話はありました。 当時はそのようなものを使ったことはありませんでしが、せっかく学習し直すのでその辺もカバーしていこうと思います。"
  },
  {
    "objectID": "posts/2025-04-14-r-blog-with-r.html#r-studioのインストール",
    "href": "posts/2025-04-14-r-blog-with-r.html#r-studioのインストール",
    "title": "RでRのブログを作る",
    "section": "R Studioのインストール",
    "text": "R Studioのインストール\n今、RをやるならR Studioを使うのがいいかなと思います。 他の選択肢もあったらすみません。\nR Studioは下記URLからダウンロードできます。 https://posit.co/download/rstudio-desktop/\n私の環境はMacなのでwindowsの方は適宜読み替えてください。"
  },
  {
    "objectID": "posts/2025-04-14-r-blog-with-r.html#今回の目的と使用技術",
    "href": "posts/2025-04-14-r-blog-with-r.html#今回の目的と使用技術",
    "title": "RでRのブログを作る",
    "section": "今回の目的と使用技術",
    "text": "今回の目的と使用技術\n今回は無料でRを使ってブログを公開することを目的にします。 ブログ内ではRのコードと実行結果が書きやすく、見やすいようにします。\nブログなどサイトを公開する際は、サーバーが必要になります。 今回はGitHub Pagesを使用します。 無料で設定が簡単そうなのが選定理由です。\nRでのブログ作成には大きく２つの方法があります。 Hugo（+ blogdown） と Quartoです。 私が調べた限り、Hugoの方がテーマ(デザイン)が豊富でサイト全体が軽量なようです。 Quartoはよりモダンで、R以外の言語もより柔軟に対応できるようです。\nHugoを使ってみようと挑戦していたのですが、うまくいかなかったので今回はQuartoはよりモダンでを使用することにしました。"
  },
  {
    "objectID": "posts/2025-04-14-r-blog-with-r.html#前準備",
    "href": "posts/2025-04-14-r-blog-with-r.html#前準備",
    "title": "RでRのブログを作る",
    "section": "前準備",
    "text": "前準備\nGitHubのアカウントが無い方は、下記から作成してください。 https://github.com\nGitHubでリポジトリを作成してください。 名前；リポジトリ名はそのままURLになります(オリジナルドメインへの変更も可能) 公開設定；Public(無料でやる場合、Publicにする必要があります) 初期ファイル；README.md を生成にチェック\n注意；下書きの記事を含めて世界中からアクセス可能な状態になるので、個人情報などは書かないようにしましょう。\n上記で作成したGitHub リポジトリをローカルにクローンしてください。\ngit clone https://github.com/yourusername/yourrepositoryname.git\ncd yourrepositoryname\nyourusername: あなたのGitHubのユーザー名\nyourrepositoryname: 先ほど作成したリポジトリ名\nRStudioを起動して、クローンしたプロジェクトを開いてください。 まず、今後の作業がしやすいようにRstudioでProjectを作成します。\n手順（RStudio内で） 1. RStudioメニュー → File → New Project… 2. 「Existing Directory（既存のフォルダ）」を選択 3. 「Browse…」をクリックし、フォルダを選択 4. 「Create Project」をクリック\nこの作業をしておくことで、ファイルの入出力などが作業フォルダ内で行われます。 この作業をしていないと、意図しないフォルダにファイルを出力することになります。"
  },
  {
    "objectID": "posts/2025-04-14-r-blog-with-r.html#quartoの導入",
    "href": "posts/2025-04-14-r-blog-with-r.html#quartoの導入",
    "title": "RでRのブログを作る",
    "section": "Quartoの導入",
    "text": "Quartoの導入\nQuartoはRStudioのConsoleから操作することができません。 なので、Macのターミナルを起動します。\nHomebrewでquartoをインストールします。\nbrew install --cask quarto\nインストールできたか不安であれば、下記コマンドで確認できます。\n$ quarto check\n[✓] Quarto CLI version 1.x.x\n[✓] R is installed\n[✓] RStudio is installed\nQuartoがインストールできたら、実際のプロジェクトを作成していきます。 作業ディレクトリで下記コマンドを実行します。\nquarto create-project . --type website\nそうすると、必要なファイルやフォルダが自動で作成されます。\n$ tree\n.\n├── README.md\n├── _quarto.yml\n├── about.qmd\n├── index.qmd\n├── relearning-stats.Rproj\n└── styles.css"
  },
  {
    "objectID": "posts/2025-04-14-r-blog-with-r.html#記事の作成",
    "href": "posts/2025-04-14-r-blog-with-r.html#記事の作成",
    "title": "RでRのブログを作る",
    "section": "記事の作成",
    "text": "記事の作成\nここからは記事作成のために準備に入っていきます。 まずは記事を保存するフォルダを作成します。\n｀``{bash} mkdir posts\n\n次に記事を書くファイルを作成します。\n\n```{bash}\ntouch posts/2025-04-14-r-blog-start.qmd\nファイルの作成ができたら、記事を書いていきます。 記事の最上部には記事に関する情報を固定で記載します。\n---\ntitle: \"Rブログはじめました\"\nauthor: \"著者名\"\ndate: 2025-04-14\ncategories: [\"コラム\"]\ntags: [\"Quarto\", \"R\"]\n---\n\n## はじめに\n\nQuartoでブログを始めました！\nマークダウン記法に関しては、別で調べてみてください。 私にも余裕があれば、まとめてみようと思います。 私自身書き始めたばかりなので、あまり分かっていません。\n記事が書けたら、ローカルでどんな感じかを確認してみます。\nquarto preview\nhttp://localhost:4200/ にアクセスすると現在のサイトを見ることができます。\nデザインとかをしていないので、かなりノスタルジックなサイトが出来上がります。"
  },
  {
    "objectID": "posts/2025-04-14-r-blog-with-r.html#公開に向けて",
    "href": "posts/2025-04-14-r-blog-with-r.html#公開に向けて",
    "title": "RでRのブログを作る",
    "section": "公開に向けて",
    "text": "公開に向けて\n下記コマンドで公開用のhtmlを自動で生成してくれます。\nquarto render\n出力は_siteというフォルダに作成されます。 私は、docsという名前のフォルダが良かったので、_quarto.yamlに下記を追加して、出力先を変更しました。\nproject:\n  type: website\n  output-dir: docs\n上記がスムーズに行ったら、GitHubにpushしてください。\ngit add -A\ngit commit -m \"First Commit\"\ngit push origin main"
  },
  {
    "objectID": "posts/2025-04-14-r-blog-with-r.html#github-pagesを有効化",
    "href": "posts/2025-04-14-r-blog-with-r.html#github-pagesを有効化",
    "title": "RでRのブログを作る",
    "section": "GitHub Pagesを有効化",
    "text": "GitHub Pagesを有効化\nGithubでの設定は最小限の解説です。\n\nリポジトリのページへ → Settings → Pages\n「Source」 → Deploy from a branch\nブランチ：main（またはmaster） / フォルダ：/docs -&gt; _siteフォルダを使用の場合はその名称で 4, 「Save」をクリック\n\nこれで一旦、サイトの公開までできます。 デザインとかまだまだ変更するべき点がありますが、一旦それは置いておきましょう。\nでは、今日はここまで。"
  },
  {
    "objectID": "posts/2025-04-15-r-iner-data.html",
    "href": "posts/2025-04-15-r-iner-data.html",
    "title": "Rの組み込みデータ",
    "section": "",
    "text": "大学を卒業してから定期的に「Rに触りたいな」と思うことはありましが、触る目的もデータもないので重い腰が上がらないまま月日が流れてしまいました。\n改めてRを触ることに決めたので、サンプルデータをどうしようかと思ったところ、Rに一定のサンプルデータがあることを思い出しました。\nそもそもどんなデータがあったのかを忘れてしまったので、どのようなデータがあったのか確認をしつつ今後の分析計画を決めたいと思います。\nまずRに組み込まれているサンプルデータを確認してみます。\n\ndata()\n\n出力数が多いので、RStudioでは別タブで表示されると思います。 このデータはRに組み込まれているので、別でパッケージなどをインストールする必要はありません。\n全データセットがかなりの数になるのでここで書くのは避けておきます。\nこの記事でどの分析に、どのデータセットがおすすめかをまとめたかったのですが、数が多く全てのデータを見ることができなかったので、今後私が触っていく中でこの記事に追記して内容を充実させていこうと思います。\nここでは最もよく使われるデータセットの１つである iris でデータの中身を確認します。\nまずはirisにどのような列があるかを確認してみます。\n\ncolnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n\nとりあえず、５つの列を確認できました。 英語なので、なんとなくどんな列かわかります。\n列名だけではデータがよく分からないので、Rの標準関数であるhead()で先頭の５行のデータを見てみましょう。\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nちなみに、引数を指定することで、表示する行数を指定することもできます。\n\nhead(iris, 10)     # 上から10行\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n\nhead(iris, n = 3)  # 上から3行（引数を明示）\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n\nちなみに、末尾からの取得も可能です。\n\ntail(iris)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica\n\n\n実際にデータを見てみることも大切ですが、データの各項目(列)が意味すること路は調べた方が早いです。\nちなみにirisデータは下記のようになっています。 irisはアヤメという植物のことです。\n\n\n\n\n\n\n\n\n\n列名\n説明\nデータ型\n単位\n\n\n\n\nSepal.Length\nがく片（Sepal）の長さ\nnumeric（数値）\nセンチメートル\n\n\nSepal.Width\nがく片（Sepal）の幅\nnumeric（数値）\nセンチメートル\n\n\nPetal.Length\n花弁（Petal）の長さ\nnumeric（数値）\nセンチメートル\n\n\nPetal.Width\n花弁（Petal）の幅\nnumeric（数値）\nセンチメートル\n\n\nSpecies\nアヤメの品種（3種類のカテゴリ）\nfactor（因子型）\n-\n\n\n\n今後、他の分析で適宜、組み込みデータを使うことがあります。 そんな時に「このデータ何！？」ってならないでもらえれば大丈夫です。"
  },
  {
    "objectID": "posts/2025-04-15-r-iner-data.html#r学習の際のサンプルデータ",
    "href": "posts/2025-04-15-r-iner-data.html#r学習の際のサンプルデータ",
    "title": "Rの組み込みデータ",
    "section": "",
    "text": "大学を卒業してから定期的に「Rに触りたいな」と思うことはありましが、触る目的もデータもないので重い腰が上がらないまま月日が流れてしまいました。\n改めてRを触ることに決めたので、サンプルデータをどうしようかと思ったところ、Rに一定のサンプルデータがあることを思い出しました。\nそもそもどんなデータがあったのかを忘れてしまったので、どのようなデータがあったのか確認をしつつ今後の分析計画を決めたいと思います。\nまずRに組み込まれているサンプルデータを確認してみます。\n\ndata()\n\n出力数が多いので、RStudioでは別タブで表示されると思います。 このデータはRに組み込まれているので、別でパッケージなどをインストールする必要はありません。\n全データセットがかなりの数になるのでここで書くのは避けておきます。\nこの記事でどの分析に、どのデータセットがおすすめかをまとめたかったのですが、数が多く全てのデータを見ることができなかったので、今後私が触っていく中でこの記事に追記して内容を充実させていこうと思います。\nここでは最もよく使われるデータセットの１つである iris でデータの中身を確認します。\nまずはirisにどのような列があるかを確認してみます。\n\ncolnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n\nとりあえず、５つの列を確認できました。 英語なので、なんとなくどんな列かわかります。\n列名だけではデータがよく分からないので、Rの標準関数であるhead()で先頭の５行のデータを見てみましょう。\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nちなみに、引数を指定することで、表示する行数を指定することもできます。\n\nhead(iris, 10)     # 上から10行\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n\nhead(iris, n = 3)  # 上から3行（引数を明示）\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n\nちなみに、末尾からの取得も可能です。\n\ntail(iris)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica\n\n\n実際にデータを見てみることも大切ですが、データの各項目(列)が意味すること路は調べた方が早いです。\nちなみにirisデータは下記のようになっています。 irisはアヤメという植物のことです。\n\n\n\n\n\n\n\n\n\n列名\n説明\nデータ型\n単位\n\n\n\n\nSepal.Length\nがく片（Sepal）の長さ\nnumeric（数値）\nセンチメートル\n\n\nSepal.Width\nがく片（Sepal）の幅\nnumeric（数値）\nセンチメートル\n\n\nPetal.Length\n花弁（Petal）の長さ\nnumeric（数値）\nセンチメートル\n\n\nPetal.Width\n花弁（Petal）の幅\nnumeric（数値）\nセンチメートル\n\n\nSpecies\nアヤメの品種（3種類のカテゴリ）\nfactor（因子型）\n-\n\n\n\n今後、他の分析で適宜、組み込みデータを使うことがあります。 そんな時に「このデータ何！？」ってならないでもらえれば大丈夫です。"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "R.html",
    "href": "R.html",
    "title": "R",
    "section": "",
    "text": "Rに関する記事をジャンル分けしてまとめたものです。\n\n\n\nRの組み込みデータ\n基礎統計量と欠損値の確認\n基本的なデータの可視化をしてみる\nt検定をする前に\nt検定\n分散分析 - 導入と独立一元配置\n分散分析 - 独立二元配置\n分散分析 - 繰り返しのある二元配置"
  },
  {
    "objectID": "R.html#r入門",
    "href": "R.html#r入門",
    "title": "R",
    "section": "",
    "text": "Rの組み込みデータ\n基礎統計量と欠損値の確認\n基本的なデータの可視化をしてみる\nt検定をする前に\nt検定\n分散分析 - 導入と独立一元配置\n分散分析 - 独立二元配置\n分散分析 - 繰り返しのある二元配置"
  }
]